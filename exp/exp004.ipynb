{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "- 推論時のtokenizeと学習時のtokenizeのやり方を合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"e04-match-tokenize\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "DATA_PATH = \"../\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME}\"\n",
    "LOG_PATH = f\"pll_data_detection/log/{EXP_NAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"pll_data_detection/trained_models/{EXP_NAME}\"\n",
    "DEBUG = True\n",
    "UPLOAD_DATA = False\n",
    "TRAINING_MAX_LENGTH = 1024  # NOTE: ほとんどOなのに、後半のtokenに正解があると、truncationで落ちてしまって勿体無い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Downloading polars-0.20.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.8/26.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: polars\n",
      "Successfully installed polars-0.20.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.37.2\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.37.2) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.37.2) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.37.2) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.37.2) (1.26.14)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets==2.16.1\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.70.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (2.28.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (10.0.1)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (4.64.1)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.20.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (1.5.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.8.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.16.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.16.1) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.16.1) (1.14.0)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "Successfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting evaluate==0.4.1\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (23.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2.16.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (4.64.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.70.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (1.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (10.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate==0.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate==0.4.1) (2022.7.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (18.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.4.1) (1.14.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting seqeval==1.2.2\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from seqeval==1.2.2) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval==1.2.2) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.9.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=7c69d00ac26bb9cc76088a95531bb3480ae7aeadccb285ad010ad9770753231f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/d6/00/1ccfd5a7466a94774e00022683d4b028836032dfb85007822b\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2019.11.28)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.27.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.6.tar.gz (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: python-slugify in /usr/lib/python3/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.6-py3-none-any.whl size=111945 sha256=3b71062a02fbf5b46be509bd5ba60011e5a32b1467da77b53d24fc9c6933af96\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/48/10/0b9ad914f1720a0f2302eabf7ded6555d52b207aa41e6b43b6\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"iinstall\" - maybe you meant \"install\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting black\n",
      "  Downloading black-24.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting isort\n",
      "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from black) (4.4.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from black) (2.0.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.9/dist-packages (from black) (2.6.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from black) (8.1.3)\n",
      "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.9/dist-packages (from black) (23.0)\n",
      "Installing collected packages: pathspec, mypy-extensions, isort, black\n",
      "Successfully installed black-24.2.0 isort-5.13.2 mypy-extensions-1.0.0 pathspec-0.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars\n",
    "%pip install transformers==4.37.2\n",
    "%pip install datasets==2.16.1\n",
    "%pip install evaluate==0.4.1\n",
    "%pip install seqeval==1.2.2\n",
    "%pip install accelerate\n",
    "%pip install python-dotenv\n",
    "%pip install kaggle\n",
    "%pip iinstall wandb==0.16.3\n",
    "\n",
    "# formatter\n",
    "%pip install black isort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TokenClassificationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert transformers.__version__ == \"4.37.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "assert datasets.__version__ == \"2.16.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "assert evaluate.__version__ == \"0.4.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if not DEBUG:\n",
    "    load_dotenv(f\"{DATA_PATH}/.env\")\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"pll\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 14 22:43:43 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   39C    P8    28W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ確認用\n",
    "train = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_807, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>86</td><td>&quot;Cheese Startup…</td><td>[&quot;Cheese&quot;, &quot;Startup&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>93</td><td>&quot;Silvia Villalo…</td><td>[&quot;Silvia&quot;, &quot;Villalobos&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>104</td><td>&quot;Storytelling  …</td><td>[&quot;Storytelling&quot;, &quot; &quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>112</td><td>&quot;Reflection – L…</td><td>[&quot;Reflection&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>123</td><td>&quot;Gandhi Institu…</td><td>[&quot;Gandhi&quot;, &quot;Institute&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>136</td><td>&quot;Dear Corrector…</td><td>[&quot;Dear&quot;, &quot;Corrector&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>166</td><td>&quot;Pepa Medrano\n",
       "\n",
       "…</td><td>[&quot;Pepa&quot;, &quot;Medrano&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>22663</td><td>&quot;Reﬂection Stor…</td><td>[&quot;Reﬂection&quot;, &quot;Storytelling&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22664</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22668</td><td>&quot;Visualization …</td><td>[&quot;Visualization&quot;, &quot;Tool&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22672</td><td>&quot;Challenge:    …</td><td>[&quot;Challenge&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22674</td><td>&quot;Reflection – V…</td><td>[&quot;Reflection&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22675</td><td>&quot;Example Reflec…</td><td>[&quot;Example&quot;, &quot;Reflection&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22677</td><td>&quot;Mind Mapping T…</td><td>[&quot;Mind&quot;, &quot;Mapping&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22678</td><td>&quot;EXAMPLE – JOUR…</td><td>[&quot;EXAMPLE&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22679</td><td>&quot;Why Mind Mappi…</td><td>[&quot;Why&quot;, &quot;Mind&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22681</td><td>&quot;Challenge\n",
       "\n",
       "So,…</td><td>[&quot;Challenge&quot;, &quot;\n",
       "\n",
       "&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22684</td><td>&quot;Brainstorming\n",
       "…</td><td>[&quot;Brainstorming&quot;, &quot;\n",
       "\n",
       "&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22687</td><td>&quot;Mind Mapping\n",
       "\n",
       "…</td><td>[&quot;Mind&quot;, &quot;Mapping&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6_807, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ …        ┆ …                   ┆ …                   ┆ …                   ┆ …                   │\n",
       "│ 22678    ┆ EXAMPLE – JOURNEY   ┆ [\"EXAMPLE\", \"–\", …  ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ MAP                 ┆ \"                   ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ THE CHALL…          ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22679    ┆ Why Mind Mapping?   ┆ [\"Why\", \"Mind\", … \" ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ Mind maps are…      ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22681    ┆ Challenge           ┆ [\"Challenge\", \"     ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ So, a few months    ┆ \", … \"              ┆                     ┆                     │\n",
       "│          ┆ back…               ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22684    ┆ Brainstorming       ┆ [\"Brainstorming\", \" ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ Challenge & Selec…  ┆ \", … \"              ┆                     ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22687    ┆ Mind Mapping        ┆ [\"Mind\", \"Mapping\", ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆ Challenge           ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│          ┆ My cons…            ┆                     ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ確認用\n",
    "test = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d6820c9d52483bb9399f86870e2094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eba8ee1a2648309aa78696e0e7715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": f\"{DATA_PATH}/train.json\"}, split=\"train\"\n",
    ")\n",
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files={\"test\": f\"{DATA_PATH}/test.json\"}, split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if DEBUG:\n",
    "    train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'trailing_whitespace', 'tokens', 'labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'tokens', 'trailing_whitespace', 'full_text'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f215ab5967b41db9bcb544daa618290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d688f5cd8f04bc5adb514ff2148bccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659dba34cc774d8881dfc0cbf19c3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '▁-',\n",
       " '▁Avril',\n",
       " '▁2021']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 512?が上限であるのに対し、NER対象のテキストが512よりも長い場合が多いため、適したモデルにする\n",
    "# DeBERTaあたりは、512を超えるテキストに対応していたような\n",
    "example = train_dataset[0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db1900150f471eae35fbe5c5a5c551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labelを変換する\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-EMAIL\",\n",
    "    4: \"I-EMAIL\",\n",
    "    5: \"B-USERNAME\",\n",
    "    6: \"I-USERNAME\",\n",
    "    7: \"B-ID_NUM\",\n",
    "    8: \"I-ID_NUM\",\n",
    "    9: \"B-PHONE_NUM\",\n",
    "    10: \"I-PHONE_NUM\",\n",
    "    11: \"B-URL_PERSONAL\",\n",
    "    12: \"I-URL_PERSONAL\",\n",
    "    13: \"B-STREET_ADDRESS\",\n",
    "    14: \"I-STREET_ADDRESS\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "def label2id_func(example):\n",
    "    example[\"labels\"] = [label2id[tag] for tag in example[\"labels\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "labele2id_train_dataset = train_dataset.map(label2id_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-EMAIL',\n",
       " 'I-EMAIL',\n",
       " 'B-USERNAME',\n",
       " 'I-USERNAME',\n",
       " 'B-ID_NUM',\n",
       " 'I-ID_NUM',\n",
       " 'B-PHONE_NUM',\n",
       " 'I-PHONE_NUM',\n",
       " 'B-URL_PERSONAL',\n",
       " 'I-URL_PERSONAL',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'I-STREET_ADDRESS']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id.keys())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1668f81acc44763b7566a305942152a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    tokenizeした文字列をinput_idsとして追加する\n",
    "    labelsには、special tokenが-100として扱われたデータが入る\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=TRAINING_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        # トークン化されたシーケンス内の各トークンが元のテキスト内のどの単語に対応するかを示すIDを提供します。\n",
    "        # 例えば、元のテキストが \"Hello, world!\" で、トークン化されたシーケンスが [\"Hello\", \",\", \"world\", \"!\"] の場合\n",
    "        # `word_ids`メソッドは [0, None, 1, None] を返します。\n",
    "        # これは、\"Hello\" が最初の単語（インデックス0）、\",\" が単語に属さない（None）、\"world\" が2番目の単語（インデックス1）、\"!\" が単語に属さない（None）ことを示します。\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif (\n",
    "                word_idx != previous_word_idx\n",
    "            ):  # subwordの場合、同じword_idxが連続している。最初のtoken以外は-100にする\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train_dataset = labele2id_train_dataset.map(\n",
    "    tokenize_and_align_labels, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'trailing_whitespace', 'tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60a20d8fec44419b0bcf62b7ede16f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f5_score(precision: float, recall: float, beta: int = 5):\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # seqevalのmetrics関数を使用して、精度、再現率、F1スコア、正解率を計算\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"f5score\": f5_score(precision, recall),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3f1ecbfd504312a2060e4f3ee5d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalデータの用意\n",
    "split_dataset = tokenized_train_dataset.train_test_split(test_size=0.2)\n",
    "tokenized_train_valid_dataset = DatasetDict(\n",
    "    {\"train\": split_dataset[\"train\"], \"valid\": split_dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'full_text', 'trailing_whitespace', 'tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5445\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['document', 'full_text', 'trailing_whitespace', 'tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LOG_PATH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # 32はだめ、性能がepoch0の時、precision0.31→0.00、Recall0.25→0.00に落ちる\n",
    "    per_device_eval_batch_size=16,  # 32,↑同様、バッチサイズが非常に重要なパラメーターであるとも言える\n",
    "    # num_train_epochs=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f5score\",  # add\n",
    "    greater_is_better=True,  # add\n",
    "    warmup_ratio=0.1,  # add\n",
    "    lr_scheduler_type=\"cosine\",  # add\n",
    "    report_to=REPORT_TO,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_valid_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_train_valid_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # モデルの保存\n",
    "# trainer.train()\n",
    "# cv_score = trainer.evaluate()[\"eval_f5score\"]\n",
    "# trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    data_collator=data_collator, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset.select([3])[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset.select([3])[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_valid_dataset[\"valid\"].select([3])[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_valid_dataset[\"valid\"].select([3])[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [203], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:3171\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3168\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3170\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3171\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3174\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:3274\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3272\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3273\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3275\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3276\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py:362\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     batch[label_name] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    359\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pad_token_id] \u001b[38;5;241m*\u001b[39m (sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)) \u001b[38;5;241m+\u001b[39m to_list(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels\n\u001b[1;32m    360\u001b[0m     ]\n\u001b[0;32m--> 362\u001b[0m batch[label_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "trainer.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CV DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_preds(trainer, valid_dataset):\n",
    "    \"\"\"\n",
    "    trainerを用いてvalid_datasetに対する予測を行う\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(valid_dataset).predictions\n",
    "    preds_final = predictions.argmax(-1)\n",
    "\n",
    "    return preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    # tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "    tokenized = tokenizer(\n",
    "        \"\".join(text),\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True\n",
    "    )\n",
    "       \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_part(preds_final, valid_dataset):\n",
    "    triplets = []\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for p, token_map, offsets, tokens, doc in zip(\n",
    "        preds_final,\n",
    "        valid_dataset[\"token_map\"],\n",
    "        valid_dataset[\"offset_mapping\"],\n",
    "        valid_dataset[\"tokens\"],\n",
    "        valid_dataset[\"document\"],\n",
    "    ):\n",
    "        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "            label_pred = id2label[token_pred]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                if triplet not in triplets:\n",
    "                    document.append(doc)\n",
    "                    token.append(token_id)\n",
    "                    label.append(label_pred)\n",
    "                    token_str.append(tokens[token_id])\n",
    "                    triplets.append(triplet)\n",
    "\n",
    "    return document, token, label, token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_df(train: pl.DataFrame):\n",
    "    # 学習データから、outputと同様のデータフレームを作成する\n",
    "    outputs = []\n",
    "    for document_id, token, label in zip(\n",
    "        train[\"document\"], train[\"tokens\"], train[\"labels\"]\n",
    "    ):\n",
    "        for token, (token_str, label_one) in enumerate(zip(token, label)):\n",
    "            if label_one != \"O\":\n",
    "                outputs.append((document_id, token, token_str, label_one))\n",
    "    return pl.DataFrame(outputs, schema=[\"document\", \"token\", \"label\", \"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_pred_join_df(\n",
    "    train_correct_df: pl.DataFrame, valid_pred_df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    validで利用したdocumentのみを抽出し、train_correct_dfとvalid_pred_dfを結合して、documentごとに比較できるようにする\n",
    "    \"\"\"\n",
    "    out = train_correct_df.filter(\n",
    "        pl.col(\"document\").is_in(valid_pred_df[\"document\"])\n",
    "    ).join(valid_pred_df, on=[\"document\", \"token\"], how=\"outer\", suffix=\"_pred\")\n",
    "\n",
    "    joined_dfs = []\n",
    "    for document in out[\"document\"].unique().to_list():\n",
    "        if document is None:\n",
    "            continue\n",
    "        joined_df_per_document = out.filter(\n",
    "            (pl.col(\"document\") == document) | (pl.col(\"document_pred\") == document)\n",
    "        )\n",
    "        joined_dfs.append(joined_df_per_document)\n",
    "\n",
    "    return pl.concat(joined_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['trailing_whitespace', 'full_text', 'labels', 'document', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'token_map'],\n",
       "    num_rows: 1362\n",
       "})"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e639fc85894a41a9e4fabb87dac955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/1362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [195], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m valid_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      5\u001b[0m     tokenize, fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer}, num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# valid_dataset = valid_dataset.map(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m valid_preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_valid_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m document, token, label, token_str \u001b[38;5;241m=\u001b[39m get_output_part(valid_preds, valid_dataset)\n\u001b[1;32m     16\u001b[0m valid_pred_df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     17\u001b[0m     [document, token, label, token_str],\n\u001b[1;32m     18\u001b[0m     schema\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn [154], line 5\u001b[0m, in \u001b[0;36mget_valid_preds\u001b[0;34m(trainer, valid_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_valid_preds\u001b[39m(trainer, valid_dataset):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    trainerを用いてvalid_datasetに対する予測を行う\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m      6\u001b[0m     preds_final \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds_final\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:3171\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3168\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3170\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3171\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3174\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:3274\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3272\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3273\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3275\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3276\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py:362\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     batch[label_name] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    359\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pad_token_id] \u001b[38;5;241m*\u001b[39m (sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)) \u001b[38;5;241m+\u001b[39m to_list(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels\n\u001b[1;32m    360\u001b[0m     ]\n\u001b[0;32m--> 362\u001b[0m batch[label_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# main\n",
    "valid_dataset = train_dataset.filter(lambda example: example['document'] in tokenized_train_valid_dataset[\"valid\"][\"document\"])\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    ")\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3\n",
    "# )\n",
    "\n",
    "valid_preds = get_valid_preds(trainer, valid_dataset)\n",
    "\n",
    "document, token, label, token_str = get_output_part(valid_preds, valid_dataset)\n",
    "\n",
    "valid_pred_df = pl.DataFrame(\n",
    "    [document, token, label, token_str],\n",
    "    schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    ")\n",
    "\n",
    "train_correct_df = make_correct_df(train)\n",
    "\n",
    "valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)\n",
    "\n",
    "# wandbにuploadする\n",
    "if not DEBUG:\n",
    "    tbl = wandb.Table(data=valid_correct_pred_df.to_pandas())\n",
    "    wandb.log({\"valid_correct_pred_df\": tbl})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(\"cp /notebooks/pll_data_detection/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataset name:e03-use-deberta-microsoft/deberta-v3-base, output_dir:pll_data_detection/trained_models/e03-use-deberta\n",
      "Starting upload for file training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.17k/4.17k [00:00<00:00, 7.49kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: training_args.bin (4KB)\n",
      "Starting upload for file tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.26M/8.26M [00:00<00:00, 13.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer.json (8MB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.57k/1.57k [00:00<00:00, 2.51kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "Starting upload for file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23.0/23.0 [00:00<00:00, 44.9B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: added_tokens.json (23B)\n",
      "Starting upload for file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.25k/1.25k [00:00<00:00, 2.74kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer_config.json (1KB)\n",
      "Starting upload for file model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 701M/701M [00:11<00:00, 63.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: model.safetensors (701MB)\n",
      "Starting upload for file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [00:00<00:00, 568B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: special_tokens_map.json (286B)\n",
      "Starting upload for file spm.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.35M/2.35M [00:00<00:00, 2.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: spm.model (2MB)\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    if \"_\" in dataset_name:\n",
    "        raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "    dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "    dataset_metadata[\"title\"] = dataset_name\n",
    "    with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "\n",
    "if (not DEBUG) and UPLOAD_DATA:\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pll_data_detection/trained_models/e03-use-deberta'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
