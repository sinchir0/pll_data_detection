{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "- 推論時のtokenizeと学習時のtokenizeのやり方を合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"e04-match-tokenize\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "DATA_PATH = \"pll_data_detection/data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME}\"\n",
    "LOG_PATH = f\"pll_data_detection/log/{EXP_NAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"pll_data_detection/trained_models/{EXP_NAME}\"\n",
    "DEBUG = False\n",
    "UPLOAD_DATA = True\n",
    "TRAINING_MAX_LENGTH = 1024  # NOTE: ほとんどOなのに、後半のtokenに正解があると、truncationで落ちてしまって勿体無い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /usr/local/lib/python3.9/dist-packages (0.20.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.9/dist-packages (4.37.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (1.23.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.9/dist-packages (from transformers==4.37.2) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.37.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.37.2) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.37.2) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.37.2) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets==2.16.1 in /usr/local/lib/python3.9/dist-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.20.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.70.13)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (2.28.2)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.16.1) (0.3.5.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.16.1) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.16.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.16.1) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.16.1) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.9/dist-packages (0.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.20.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.70.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (1.5.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (1.23.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2023.10.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (23.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate==0.4.1) (2.28.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (5.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.1) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate==0.4.1) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate==0.4.1) (2.8.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.4.1) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from seqeval==1.2.2) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval==1.2.2) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.9.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.27.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.6.6)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)\n",
      "Requirement already satisfied: python-slugify in /usr/lib/python3/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.28.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "ERROR: unknown command \"iinstall\" - maybe you meant \"install\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.9/dist-packages (24.2.0)\n",
      "Requirement already satisfied: isort in /usr/local/lib/python3.9/dist-packages (5.13.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from black) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from black) (0.12.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.9/dist-packages (from black) (23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from black) (8.1.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from black) (2.0.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.9/dist-packages (from black) (2.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from black) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars\n",
    "%pip install transformers==4.37.2\n",
    "%pip install datasets==2.16.1\n",
    "%pip install evaluate==0.4.1\n",
    "%pip install seqeval==1.2.2\n",
    "%pip install accelerate\n",
    "%pip install python-dotenv\n",
    "%pip install kaggle\n",
    "%pip iinstall wandb==0.16.3\n",
    "\n",
    "# formatter\n",
    "%pip install black isort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TokenClassificationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert transformers.__version__ == \"4.37.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "assert datasets.__version__ == \"2.16.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "assert evaluate.__version__ == \"0.4.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinchir0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240217_072859-2yup877d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sinchir0/pll/runs/2yup877d\" target=\"_blank\">e04-match-tokenize</a></strong> to <a href=\"https://wandb.ai/sinchir0/pll\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if not DEBUG:\n",
    "    load_dotenv(\"pll_data_detection/.env\")\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"pll\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 17 07:29:26 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   55C    P8    36W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ確認用\n",
    "train = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_807, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>86</td><td>&quot;Cheese Startup…</td><td>[&quot;Cheese&quot;, &quot;Startup&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>93</td><td>&quot;Silvia Villalo…</td><td>[&quot;Silvia&quot;, &quot;Villalobos&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>104</td><td>&quot;Storytelling  …</td><td>[&quot;Storytelling&quot;, &quot; &quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>112</td><td>&quot;Reflection – L…</td><td>[&quot;Reflection&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>123</td><td>&quot;Gandhi Institu…</td><td>[&quot;Gandhi&quot;, &quot;Institute&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>136</td><td>&quot;Dear Corrector…</td><td>[&quot;Dear&quot;, &quot;Corrector&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>166</td><td>&quot;Pepa Medrano\n",
       "\n",
       "…</td><td>[&quot;Pepa&quot;, &quot;Medrano&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>22663</td><td>&quot;Reﬂection Stor…</td><td>[&quot;Reﬂection&quot;, &quot;Storytelling&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22664</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22668</td><td>&quot;Visualization …</td><td>[&quot;Visualization&quot;, &quot;Tool&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22672</td><td>&quot;Challenge:    …</td><td>[&quot;Challenge&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22674</td><td>&quot;Reflection – V…</td><td>[&quot;Reflection&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22675</td><td>&quot;Example Reflec…</td><td>[&quot;Example&quot;, &quot;Reflection&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22677</td><td>&quot;Mind Mapping T…</td><td>[&quot;Mind&quot;, &quot;Mapping&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22678</td><td>&quot;EXAMPLE – JOUR…</td><td>[&quot;EXAMPLE&quot;, &quot;–&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22679</td><td>&quot;Why Mind Mappi…</td><td>[&quot;Why&quot;, &quot;Mind&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22681</td><td>&quot;Challenge\n",
       "\n",
       "So,…</td><td>[&quot;Challenge&quot;, &quot;\n",
       "\n",
       "&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22684</td><td>&quot;Brainstorming\n",
       "…</td><td>[&quot;Brainstorming&quot;, &quot;\n",
       "\n",
       "&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>22687</td><td>&quot;Mind Mapping\n",
       "\n",
       "…</td><td>[&quot;Mind&quot;, &quot;Mapping&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6_807, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ …        ┆ …                   ┆ …                   ┆ …                   ┆ …                   │\n",
       "│ 22678    ┆ EXAMPLE – JOURNEY   ┆ [\"EXAMPLE\", \"–\", …  ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ MAP                 ┆ \"                   ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ THE CHALL…          ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22679    ┆ Why Mind Mapping?   ┆ [\"Why\", \"Mind\", … \" ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ Mind maps are…      ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22681    ┆ Challenge           ┆ [\"Challenge\", \"     ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ So, a few months    ┆ \", … \"              ┆                     ┆                     │\n",
       "│          ┆ back…               ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22684    ┆ Brainstorming       ┆ [\"Brainstorming\", \" ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆                     ┆ false]              ┆                     │\n",
       "│          ┆ Challenge & Selec…  ┆ \", … \"              ┆                     ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 22687    ┆ Mind Mapping        ┆ [\"Mind\", \"Mapping\", ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆ Challenge           ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│          ┆ My cons…            ┆                     ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ確認用\n",
    "test = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": f\"{DATA_PATH}/train.json\"}, split=\"train\"\n",
    ").rename_column(\"labels\", \"provided_labels\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files={\"test\": f\"{DATA_PATH}/test.json\"}, split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if DEBUG:\n",
    "    train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['trailing_whitespace', 'full_text', 'document', 'tokens', 'provided_labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['trailing_whitespace', 'full_text', 'tokens', 'document'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '▁-',\n",
       " '▁Avril',\n",
       " '▁2021',\n",
       " '▁-',\n",
       " '▁Nathalie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Challenge',\n",
       " '▁&',\n",
       " '▁selection',\n",
       " '▁The',\n",
       " '▁tool',\n",
       " '▁I',\n",
       " '▁use',\n",
       " '▁to',\n",
       " '▁help',\n",
       " '▁all',\n",
       " '▁stakeholders',\n",
       " '▁finding',\n",
       " '▁their',\n",
       " '▁way',\n",
       " '▁through',\n",
       " '▁the',\n",
       " '▁complexity',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁project',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁.',\n",
       " '▁What',\n",
       " '▁exactly',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁?',\n",
       " '▁According',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁definition',\n",
       " '▁of',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁T',\n",
       " '.',\n",
       " '▁and',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁B',\n",
       " '.',\n",
       " '▁(',\n",
       " '▁1999',\n",
       " '▁,',\n",
       " '▁Des',\n",
       " 's',\n",
       " 'ine',\n",
       " '▁-',\n",
       " '▁moi',\n",
       " '▁l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '▁.',\n",
       " '▁Paris',\n",
       " '▁:',\n",
       " '▁Les',\n",
       " '▁É',\n",
       " 'dition',\n",
       " 's',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '▁.',\n",
       " '▁)',\n",
       " '▁,',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁(',\n",
       " '▁or',\n",
       " '▁heuristic',\n",
       " '▁diagram',\n",
       " '▁)',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁graphic',\n",
       " '▁representation',\n",
       " '▁technique',\n",
       " '▁that',\n",
       " '▁follows',\n",
       " '▁the',\n",
       " '▁natural',\n",
       " '▁functioning',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁brain',\n",
       " \"▁'\",\n",
       " 's',\n",
       " '▁potential',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁released',\n",
       " '▁.',\n",
       " '▁Cf',\n",
       " '▁Annex',\n",
       " '1',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁has',\n",
       " '▁many',\n",
       " '▁advantages',\n",
       " '▁:',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁accessible',\n",
       " '▁to',\n",
       " '▁all',\n",
       " '▁and',\n",
       " '▁does',\n",
       " '▁not',\n",
       " '▁require',\n",
       " '▁significant',\n",
       " '▁material',\n",
       " '▁investment',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁done',\n",
       " '▁quickly',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁scalable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁categorization',\n",
       " '▁and',\n",
       " '▁linking',\n",
       " '▁of',\n",
       " '▁information',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁applied',\n",
       " '▁to',\n",
       " '▁any',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁situation',\n",
       " '▁:',\n",
       " '▁note',\n",
       " 'taking',\n",
       " '▁,',\n",
       " '▁problem',\n",
       " '▁solving',\n",
       " '▁,',\n",
       " '▁analysis',\n",
       " '▁,',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁suitable',\n",
       " '▁for',\n",
       " '▁all',\n",
       " '▁people',\n",
       " '▁and',\n",
       " '▁is',\n",
       " '▁easy',\n",
       " '▁to',\n",
       " '▁learn',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁fun',\n",
       " '▁and',\n",
       " '▁encourages',\n",
       " '▁exchanges',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁dimension',\n",
       " '▁of',\n",
       " '▁projects',\n",
       " '▁,',\n",
       " '▁opportunities',\n",
       " '▁,',\n",
       " '▁interconnections',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁synthesize',\n",
       " 's',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁understandable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁you',\n",
       " '▁to',\n",
       " '▁explore',\n",
       " '▁ideas',\n",
       " '▁The',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁starts',\n",
       " '▁with',\n",
       " '▁an',\n",
       " '▁idea',\n",
       " '▁/',\n",
       " '▁problem',\n",
       " '▁located',\n",
       " '▁at',\n",
       " '▁its',\n",
       " '▁center',\n",
       " '▁.',\n",
       " '▁This',\n",
       " '▁starting',\n",
       " '▁point',\n",
       " '▁generates',\n",
       " '▁ideas',\n",
       " '▁/',\n",
       " '▁work',\n",
       " '▁areas',\n",
       " '▁,',\n",
       " '▁incremented',\n",
       " '▁around',\n",
       " '▁this',\n",
       " '▁center',\n",
       " '▁in',\n",
       " '▁a',\n",
       " '▁radial',\n",
       " '▁structure',\n",
       " '▁,',\n",
       " '▁which',\n",
       " '▁in',\n",
       " '▁turn',\n",
       " '▁is',\n",
       " '▁completed',\n",
       " '▁with',\n",
       " '▁as',\n",
       " '▁many',\n",
       " '▁branches',\n",
       " '▁as',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁enables',\n",
       " '▁creativity',\n",
       " '▁and',\n",
       " '▁logic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁mobilized',\n",
       " '▁,',\n",
       " '▁it',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁map',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁thoughts',\n",
       " '▁.',\n",
       " '▁Creativity',\n",
       " '▁is',\n",
       " '▁enhanced',\n",
       " '▁because',\n",
       " '▁participants',\n",
       " '▁feel',\n",
       " '▁comfortable',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁method',\n",
       " '▁.',\n",
       " '▁Application',\n",
       " '▁&',\n",
       " '▁Insight',\n",
       " '▁I',\n",
       " '▁start',\n",
       " '▁the',\n",
       " '▁process',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁creation',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁standing',\n",
       " '▁around',\n",
       " '▁a',\n",
       " '▁large',\n",
       " '▁board',\n",
       " '▁(',\n",
       " '▁white',\n",
       " '▁or',\n",
       " '▁paper',\n",
       " '▁board',\n",
       " '▁)',\n",
       " '▁.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁center',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁board',\n",
       " '▁,',\n",
       " '▁I',\n",
       " '▁write',\n",
       " '▁and',\n",
       " '▁highlight',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁design',\n",
       " '▁.',\n",
       " '▁Through',\n",
       " '▁a',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " '▁,',\n",
       " '▁I',\n",
       " '▁guide',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁in',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁.',\n",
       " '▁I',\n",
       " '▁adapt',\n",
       " '▁the',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " '▁according',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁addressed',\n",
       " '▁.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " '▁,',\n",
       " '▁we',\n",
       " '▁can',\n",
       " '▁use',\n",
       " '▁:',\n",
       " '▁who',\n",
       " '▁,',\n",
       " '▁what',\n",
       " '▁,',\n",
       " '▁when',\n",
       " '▁,',\n",
       " '▁where',\n",
       " '▁,',\n",
       " '▁why',\n",
       " '▁,',\n",
       " '▁how',\n",
       " '▁,',\n",
       " '▁how',\n",
       " '▁much',\n",
       " '▁.',\n",
       " '▁The',\n",
       " '▁use',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁“',\n",
       " '▁why',\n",
       " '▁',\n",
       " '”',\n",
       " '▁is',\n",
       " '▁very',\n",
       " '▁interesting',\n",
       " '▁to',\n",
       " '▁understand',\n",
       " '▁the',\n",
       " '▁origin',\n",
       " '▁.',\n",
       " '▁By',\n",
       " '▁this',\n",
       " '▁way',\n",
       " '▁,',\n",
       " '▁the',\n",
       " '▁interviewed',\n",
       " '▁person',\n",
       " '▁free',\n",
       " 's',\n",
       " '▁itself',\n",
       " '▁from',\n",
       " '▁paradigms',\n",
       " '▁and',\n",
       " '▁thus',\n",
       " '▁dares',\n",
       " '▁to',\n",
       " '▁propose',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁/',\n",
       " '▁ways',\n",
       " '▁of',\n",
       " '▁functioning',\n",
       " '▁.',\n",
       " '▁I',\n",
       " '▁plan',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁for',\n",
       " '▁a',\n",
       " '▁workshop',\n",
       " '▁.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '▁-',\n",
       " '▁Avril',\n",
       " '▁2021',\n",
       " '▁-',\n",
       " '▁Nathalie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁After',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁on',\n",
       " '▁paper',\n",
       " '▁,',\n",
       " '▁I',\n",
       " '▁propose',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁a',\n",
       " '▁digital',\n",
       " '▁visualization',\n",
       " '▁of',\n",
       " '▁their',\n",
       " '▁work',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁addition',\n",
       " '▁of',\n",
       " '▁color',\n",
       " '▁codes',\n",
       " '▁,',\n",
       " '▁images',\n",
       " '▁and',\n",
       " '▁interconnections',\n",
       " '▁.',\n",
       " '▁This',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁also',\n",
       " '▁last',\n",
       " 's',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁to',\n",
       " '▁evolve',\n",
       " '▁.',\n",
       " '▁Once',\n",
       " '▁familiarized',\n",
       " '▁with',\n",
       " '▁it',\n",
       " '▁,',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁discover',\n",
       " '▁the',\n",
       " '▁power',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁tool',\n",
       " '▁.',\n",
       " '▁Then',\n",
       " '▁,',\n",
       " '▁the',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁brings',\n",
       " '▁out',\n",
       " '▁even',\n",
       " '▁more',\n",
       " '▁ideas',\n",
       " '▁and',\n",
       " '▁constructive',\n",
       " '▁exchanges',\n",
       " '▁between',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁.',\n",
       " '▁Around',\n",
       " '▁this',\n",
       " '▁new',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁,',\n",
       " '▁they',\n",
       " '▁have',\n",
       " '▁learned',\n",
       " '▁to',\n",
       " '▁work',\n",
       " '▁together',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁make',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁untold',\n",
       " '▁ideas',\n",
       " '▁.',\n",
       " '▁I',\n",
       " '▁now',\n",
       " '▁present',\n",
       " '▁all',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁I',\n",
       " '▁manage',\n",
       " '▁in',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁format',\n",
       " '▁in',\n",
       " '▁order',\n",
       " '▁to',\n",
       " '▁ease',\n",
       " '▁rapid',\n",
       " '▁understanding',\n",
       " '▁for',\n",
       " '▁decision',\n",
       " '▁-',\n",
       " '▁makers',\n",
       " '▁.',\n",
       " '▁These',\n",
       " '▁presentations',\n",
       " '▁are',\n",
       " '▁the',\n",
       " '▁core',\n",
       " '▁of',\n",
       " '▁my',\n",
       " '▁business',\n",
       " '▁models',\n",
       " '▁.',\n",
       " '▁The',\n",
       " '▁decision',\n",
       " '▁-',\n",
       " '▁makers',\n",
       " '▁are',\n",
       " '▁thus',\n",
       " '▁able',\n",
       " '▁to',\n",
       " '▁identify',\n",
       " '▁the',\n",
       " '▁opportunities',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁take',\n",
       " '▁quick',\n",
       " '▁decisions',\n",
       " '▁to',\n",
       " '▁validate',\n",
       " '▁them',\n",
       " '▁.',\n",
       " '▁They',\n",
       " '▁find',\n",
       " '▁answers',\n",
       " '▁to',\n",
       " '▁their',\n",
       " '▁questions',\n",
       " '▁thank',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁schematic',\n",
       " '▁representation',\n",
       " '▁.',\n",
       " '▁Approach',\n",
       " '▁What',\n",
       " '▁I',\n",
       " '▁find',\n",
       " '▁amazing',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁facilitation',\n",
       " '▁of',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁workshop',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁commitment',\n",
       " '▁for',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁helps',\n",
       " '▁to',\n",
       " '▁give',\n",
       " '▁meaning',\n",
       " '▁.',\n",
       " '▁The',\n",
       " '▁participants',\n",
       " '▁appropriate',\n",
       " '▁the',\n",
       " '▁story',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁keep',\n",
       " '▁writing',\n",
       " '▁it',\n",
       " '▁.',\n",
       " '▁Then',\n",
       " '▁,',\n",
       " '▁they',\n",
       " '▁easily',\n",
       " '▁become',\n",
       " '▁actors',\n",
       " '▁or',\n",
       " '▁sponsors',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁.',\n",
       " '▁A',\n",
       " '▁trust',\n",
       " '▁relationship',\n",
       " '▁is',\n",
       " '▁built',\n",
       " '▁,',\n",
       " '▁thus',\n",
       " '▁facilitating',\n",
       " '▁the',\n",
       " '▁implementation',\n",
       " '▁of',\n",
       " '▁related',\n",
       " '▁actions',\n",
       " '▁.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '▁-',\n",
       " '▁Avril',\n",
       " '▁2021',\n",
       " '▁-',\n",
       " '▁Nathalie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Annex',\n",
       " '▁1',\n",
       " '▁:',\n",
       " '▁Mind',\n",
       " '▁Map',\n",
       " '▁Shared',\n",
       " '▁facilities',\n",
       " '▁project',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 512?が上限であるのに対し、NER対象のテキストが512よりも長い場合が多いため、適したモデルにする\n",
    "# DeBERTaあたりは、512を超えるテキストに対応していたような\n",
    "# example = train_dataset[30]\n",
    "example = train_dataset[0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# labelを変換する\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-EMAIL\",\n",
    "    4: \"I-EMAIL\",\n",
    "    5: \"B-USERNAME\",\n",
    "    6: \"I-USERNAME\",\n",
    "    7: \"B-ID_NUM\",\n",
    "    8: \"I-ID_NUM\",\n",
    "    9: \"B-PHONE_NUM\",\n",
    "    10: \"I-PHONE_NUM\",\n",
    "    11: \"B-URL_PERSONAL\",\n",
    "    12: \"I-URL_PERSONAL\",\n",
    "    13: \"B-STREET_ADDRESS\",\n",
    "    14: \"I-STREET_ADDRESS\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "# def label2id_func(example):\n",
    "#     example[\"labels\"] = [label2id[tag] for tag in example[\"labels\"]]\n",
    "#     return example\n",
    "\n",
    "\n",
    "# labele2id_train_dataset = train_dataset.map(label2id_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdd962ec3944e05933639313e591549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "    \"\"\"\n",
    "    与えられたtokenとlabelから、\n",
    "    今回のtokenizerで区切った場合のtokenとlabelを作成する。\n",
    "    \"\"\"\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        # 文字数分だけ、該当のラベルを追加する\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "        # text -> ['Design', ' ']\n",
    "        # labels -> ['O', 'O', 'O', 'O', 'O', 'O', 'O'] (6文字分 + 空白1文字分)\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\n",
    "        \"\".join(text),\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # offset_mappingの各エントリは、トークンが元のテキストのどの範囲（開始位置と終了位置）にマッピングされるかを示す\n",
    "        # タプルまたはリストで構成されます。\n",
    "\n",
    "        # CLS tokenの対応\n",
    "        # CLSやSEPには必ず(start_idx, end_idx) = (0, 0)が割り当てられる\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # 空白が存在する時は、offset_mappingのstart_idxを+1する\n",
    "        # DeBERTaV2Tokenizerは、空白を文字の先頭に▁としてくっつけるため。\n",
    "        # NOTE: もし空白を▁として文字の先頭にくっつけるないtokenizerの場合は、不要\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    # Q: token_labelsは何の長さ？\n",
    "    # A: 今回のtokenizerで区切った時の、tokenに該当するlabel\n",
    "    # 例:\n",
    "    # 与えられたtoken example[\"tokens\"][:10]\n",
    "    # -> ['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n",
    "    # 今回のtokenizerで区切ったtoken　tokenizer.convert_ids_to_tokens(tokenized.input_ids[:10])\n",
    "    # -> ['[CLS]', '▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion', '-', 'Av', 'ril']\n",
    "    # 文字数が違う！！！\n",
    "    # 最初に与えられたtokenとそのlabelだと、今回のtokenizerで区切った場合のラベルが分からない。\n",
    "    # 今回のtokenizerで区切った場合のtokenとラベルを作成した。\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "        \"max_length\": TRAINING_MAX_LENGTH,\n",
    "    },\n",
    "    num_proc=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gilberto', 'B-NAME_STUDENT')\n",
      "('Gamboa', 'I-NAME_STUDENT')\n",
      "****************************************************************************************************\n",
      "('▁Gilberto', 'B-NAME_STUDENT')\n",
      "('▁Gamb', 'I-NAME_STUDENT')\n",
      "('oa', 'I-NAME_STUDENT')\n"
     ]
    }
   ],
   "source": [
    "x = train_dataset[2]\n",
    "\n",
    "for t, l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n",
    "    if l != \"O\":\n",
    "        print((t, l))\n",
    "\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n",
    "    if id2label[l] != \"O\":\n",
    "        print((t, id2label[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-EMAIL',\n",
       " 'I-EMAIL',\n",
       " 'B-USERNAME',\n",
       " 'I-USERNAME',\n",
       " 'B-ID_NUM',\n",
       " 'I-ID_NUM',\n",
       " 'B-PHONE_NUM',\n",
       " 'I-PHONE_NUM',\n",
       " 'B-URL_PERSONAL',\n",
       " 'I-URL_PERSONAL',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'I-STREET_ADDRESS']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id.keys())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(examples):\n",
    "#     \"\"\"\n",
    "#     tokenizeした文字列をinput_idsとして追加する\n",
    "#     labelsには、special tokenが-100として扱われたデータが入る\n",
    "#     \"\"\"\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         examples[\"tokens\"],\n",
    "#         truncation=True,\n",
    "#         is_split_into_words=True,\n",
    "#         max_length=TRAINING_MAX_LENGTH,\n",
    "#     )\n",
    "\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(examples[\"labels\"]):\n",
    "#         # トークン化されたシーケンス内の各トークンが元のテキスト内のどの単語に対応するかを示すIDを提供します。\n",
    "#         # 例えば、元のテキストが \"Hello, world!\" で、トークン化されたシーケンスが [\"Hello\", \",\", \"world\", \"!\"] の場合\n",
    "#         # `word_ids`メソッドは [0, None, 1, None] を返します。\n",
    "#         # これは、\"Hello\" が最初の単語（インデックス0）、\",\" が単語に属さない（None）、\"world\" が2番目の単語（インデックス1）、\"!\" が単語に属さない（None）ことを示します。\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)\n",
    "#             elif (\n",
    "#                 word_idx != previous_word_idx\n",
    "#             ):  # subwordの場合、同じword_idxが連続している。最初のtoken以外は-100にする\n",
    "#                 label_ids.append(label[word_idx])\n",
    "#             else:\n",
    "#                 label_ids.append(-100)\n",
    "#             previous_word_idx = word_idx\n",
    "#         labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "\n",
    "# tokenized_train_dataset = labele2id_train_dataset.map(\n",
    "#     tokenize_and_align_labels, batched=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_to_multiple_of\n",
    "# paddingの際に、指定した数の倍数になるように、各サンプルの長さを揃える\n",
    "# ハードウェアの要件に合致することで、計算効率が良くなる可能性がある\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f5_score(precision: float, recall: float, beta: int = 5):\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # seqevalのmetrics関数を使用して、精度、再現率、F1スコア、正解率を計算\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"f5score\": f5_score(precision, recall),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalデータの用意\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.2)\n",
    "train_valid_dataset = DatasetDict(\n",
    "    {\"train\": split_dataset[\"train\"], \"valid\": split_dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['trailing_whitespace', 'full_text', 'document', 'tokens', 'provided_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],\n",
       "        num_rows: 5445\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['trailing_whitespace', 'full_text', 'document', 'tokens', 'provided_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],\n",
       "        num_rows: 1362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LOG_PATH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # 32はだめ、性能がepoch0の時、precision0.31→0.00、Recall0.25→0.00に落ちる\n",
    "    per_device_eval_batch_size=16,  # 32,↑同様、バッチサイズが非常に重要なパラメーターであるとも言える\n",
    "    # num_train_epochs=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f5score\",  # add\n",
    "    greater_is_better=True,  # add\n",
    "    warmup_ratio=0.1,  # add\n",
    "    lr_scheduler_type=\"cosine\",  # add\n",
    "    report_to=REPORT_TO,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  60/1023 01:37 < 26:57, 0.60 it/s, Epoch 0.17/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nub51uvc3r.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# モデルの学習\n",
    "trainer.train()\n",
    "cv_score = trainer.evaluate()[\"eval_f5score\"]\n",
    "# モデルの保存\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \".\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(train_valid_dataset[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CV DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_preds(trainer, valid_dataset):\n",
    "    \"\"\"\n",
    "    trainerを用いてvalid_datasetに対する予測を行う\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(valid_dataset).predictions\n",
    "    preds_final = predictions.argmax(-1)\n",
    "\n",
    "    return preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx] * len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_part(preds_final, valid_dataset):\n",
    "    triplets = []\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for p, token_map, offsets, tokens, doc in zip(\n",
    "        preds_final,\n",
    "        valid_dataset[\"token_map\"],\n",
    "        valid_dataset[\"offset_mapping\"],\n",
    "        valid_dataset[\"tokens\"],\n",
    "        valid_dataset[\"document\"],\n",
    "    ):\n",
    "        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "            label_pred = id2label[token_pred]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                if triplet not in triplets:\n",
    "                    document.append(doc)\n",
    "                    token.append(token_id)\n",
    "                    label.append(label_pred)\n",
    "                    token_str.append(tokens[token_id])\n",
    "                    triplets.append(triplet)\n",
    "\n",
    "    return document, token, label, token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_df(train: pl.DataFrame):\n",
    "    # 学習データから、outputと同様のデータフレームを作成する\n",
    "    outputs = []\n",
    "    for document_id, token, label in zip(\n",
    "        train[\"document\"], train[\"tokens\"], train[\"labels\"]\n",
    "    ):\n",
    "        for token, (token_str, label_one) in enumerate(zip(token, label)):\n",
    "            if label_one != \"O\":\n",
    "                outputs.append((document_id, token, token_str, label_one))\n",
    "    return pl.DataFrame(outputs, schema=[\"document\", \"token\", \"label\", \"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_pred_join_df(\n",
    "    train_correct_df: pl.DataFrame, valid_pred_df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    validで利用したdocumentのみを抽出し、train_correct_dfとvalid_pred_dfを結合して、documentごとに比較できるようにする\n",
    "    \"\"\"\n",
    "    out = train_correct_df.filter(\n",
    "        pl.col(\"document\").is_in(valid_pred_df[\"document\"])\n",
    "    ).join(valid_pred_df, on=[\"document\", \"token\"], how=\"outer\", suffix=\"_pred\")\n",
    "\n",
    "    joined_dfs = []\n",
    "    for document in out[\"document\"].unique().to_list():\n",
    "        if document is None:\n",
    "            continue\n",
    "        joined_df_per_document = out.filter(\n",
    "            (pl.col(\"document\") == document) | (pl.col(\"document_pred\") == document)\n",
    "        )\n",
    "        joined_dfs.append(joined_df_per_document)\n",
    "\n",
    "    return pl.concat(joined_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# main\n",
    "valid_dataset = train_dataset.filter(\n",
    "    lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"]\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    ")\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3\n",
    "# )\n",
    "\n",
    "valid_preds = get_valid_preds(trainer, valid_dataset)\n",
    "\n",
    "document, token, label, token_str = get_output_part(valid_preds, valid_dataset)\n",
    "\n",
    "valid_pred_df = pl.DataFrame(\n",
    "    [document, token, label, token_str],\n",
    "    schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    ")\n",
    "\n",
    "train_correct_df = make_correct_df(train)\n",
    "\n",
    "valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)\n",
    "\n",
    "# wandbにuploadする\n",
    "if not DEBUG:\n",
    "    tbl = wandb.Table(data=valid_correct_pred_df.to_pandas())\n",
    "    wandb.log({\"valid_correct_pred_df\": tbl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_correct_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(\"cp /notebooks/pll_data_detection/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    if \"_\" in dataset_name:\n",
    "        raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "    dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "    dataset_metadata[\"title\"] = dataset_name\n",
    "    with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "\n",
    "if (not DEBUG) and UPLOAD_DATA:\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
