{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "- オリジナルデータを全て学習に使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:10:36.219874Z",
     "iopub.status.busy": "2024-03-20T06:10:36.219690Z",
     "iopub.status.idle": "2024-03-20T06:10:36.227652Z",
     "shell.execute_reply": "2024-03-20T06:10:36.227145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"e054-fine-tuning-all\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "DATA_PATH = \"pll_data_detection/data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "if len(DATASET_NAME) < 6 or len(DATASET_NAME) > 50:\n",
    "    raise Exception(f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\")\n",
    "LOG_PATH = f\"pll_data_detection/log/{EXP_NAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"pll_data_detection/trained_models/{EXP_NAME}\"\n",
    "\n",
    "DEBUG = False\n",
    "UPLOAD_DATA = True\n",
    "TRAINING = True\n",
    "\n",
    "# VALID_DATA_SIZE = 0.5\n",
    "SEED = 42\n",
    "EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:10:36.230017Z",
     "iopub.status.busy": "2024-03-20T06:10:36.229853Z",
     "iopub.status.idle": "2024-03-20T06:10:36.234013Z",
     "shell.execute_reply": "2024-03-20T06:10:36.233525Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def resolve_path(base_path: str) -> str:\n",
    "    cwd = os.getcwd()\n",
    "    if cwd == \"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode or nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == \"/notebooks/pll_data_detection/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:10:36.236010Z",
     "iopub.status.busy": "2024-03-20T06:10:36.235851Z",
     "iopub.status.idle": "2024-03-20T06:10:36.238655Z",
     "shell.execute_reply": "2024-03-20T06:10:36.238234Z"
    }
   },
   "outputs": [],
   "source": [
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:10:36.240419Z",
     "iopub.status.busy": "2024-03-20T06:10:36.240273Z",
     "iopub.status.idle": "2024-03-20T06:11:00.712987Z",
     "shell.execute_reply": "2024-03-20T06:11:00.712333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q polars==0.20.10\n",
    "%pip install -q transformers==4.37.2\n",
    "%pip install -q datasets==2.16.1\n",
    "%pip install -q evaluate==0.4.1\n",
    "%pip install -q seqeval==1.2.2\n",
    "%pip install -q accelerate\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q wandb==0.16.3\n",
    "\n",
    "# formatter\n",
    "%pip install -q black isort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:00.716186Z",
     "iopub.status.busy": "2024-03-20T06:11:00.715668Z",
     "iopub.status.idle": "2024-03-20T06:11:04.298734Z",
     "shell.execute_reply": "2024-03-20T06:11:04.297926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets, Value, ClassLabel\n",
    "from seqeval.metrics.sequence_labeling import precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:04.301779Z",
     "iopub.status.busy": "2024-03-20T06:11:04.301355Z",
     "iopub.status.idle": "2024-03-20T06:11:04.345554Z",
     "shell.execute_reply": "2024-03-20T06:11:04.344903Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "assert transformers.__version__ == \"4.37.2\"\n",
    "assert datasets.__version__ == \"2.16.1\"\n",
    "assert evaluate.__version__ == \"0.4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:04.348722Z",
     "iopub.status.busy": "2024-03-20T06:11:04.348506Z",
     "iopub.status.idle": "2024-03-20T06:11:04.353326Z",
     "shell.execute_reply": "2024-03-20T06:11:04.352858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:04.355376Z",
     "iopub.status.busy": "2024-03-20T06:11:04.355205Z",
     "iopub.status.idle": "2024-03-20T06:11:08.148835Z",
     "shell.execute_reply": "2024-03-20T06:11:08.148261Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if not DEBUG:\n",
    "    load_dotenv(f\"{DATA_PATH}/.env\")\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"pll\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:08.178944Z",
     "iopub.status.busy": "2024-03-20T06:11:08.178452Z",
     "iopub.status.idle": "2024-03-20T06:11:08.768316Z",
     "shell.execute_reply": "2024-03-20T06:11:08.767410Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:08.771805Z",
     "iopub.status.busy": "2024-03-20T06:11:08.771149Z",
     "iopub.status.idle": "2024-03-20T06:11:09.641616Z",
     "shell.execute_reply": "2024-03-20T06:11:09.640801Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:09.644780Z",
     "iopub.status.busy": "2024-03-20T06:11:09.644476Z",
     "iopub.status.idle": "2024-03-20T06:11:09.832743Z",
     "shell.execute_reply": "2024-03-20T06:11:09.831996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    load_dataset(\n",
    "        \"json\", data_files=f\"{DATA_PATH}/train.json\", split=\"train\"\n",
    "    ).rename_column(\"labels\", \"provided_labels\")\n",
    "    # documentをstringに変換\n",
    "    .cast_column(\"document\", Value(dtype=\"string\", id=None))\n",
    "    # 識別のためのflagを追加\n",
    "    .map(lambda example: {\"flag\": \"original\"})\n",
    ")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files={\"test\": f\"{DATA_PATH}/test.json\"}, split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:09.835705Z",
     "iopub.status.busy": "2024-03-20T06:11:09.835213Z",
     "iopub.status.idle": "2024-03-20T06:11:11.392344Z",
     "shell.execute_reply": "2024-03-20T06:11:11.391654Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 外部データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.395408Z",
     "iopub.status.busy": "2024-03-20T06:11:11.394817Z",
     "iopub.status.idle": "2024-03-20T06:11:11.680408Z",
     "shell.execute_reply": "2024-03-20T06:11:11.679824Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_string_to_list(example, col):\n",
    "    # 'your_column_name'は変換したい列の名前に置き換えてください\n",
    "    example[col] = eval(example[col])\n",
    "    return example\n",
    "\n",
    "\n",
    "# external_pii_dataset = (\n",
    "#     load_dataset(\n",
    "#         \"csv\",\n",
    "#         data_files={\"train\": f\"{DATA_PATH}/pii_dataset_fixed.csv\"},\n",
    "#         split=\"train\",\n",
    "#     )\n",
    "#     .select_columns([\"document\", \"labels\", \"text\", \"trailing_whitespace\", \"tokens\"])\n",
    "#     .rename_columns({\"labels\": \"provided_labels\", \"text\": \"full_text\"})\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"provided_labels\"}, num_proc=3)\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"trailing_whitespace\"}, num_proc=3)\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"tokens\"}, num_proc=3)\n",
    "#     .map(lambda example: {\"flag\": \"external\"}, num_proc=3)\n",
    "# )\n",
    "\n",
    "moredata_pii_dataset = (\n",
    "    load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\"train\": f\"{DATA_PATH}/moredata_dataset_fixed.csv\"},\n",
    "        split=\"train\",\n",
    "    )\n",
    "    .rename_columns({\"labels\": \"provided_labels\", \"text\": \"full_text\"})\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"provided_labels\"}, num_proc=3)\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"trailing_whitespace\"}, num_proc=3)\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"tokens\"}, num_proc=3)\n",
    "    .map(lambda example: {\"flag\": \"moredata\"}, num_proc=3)\n",
    ")\n",
    "\n",
    "# mixtral\n",
    "mixtral = (\n",
    "    load_dataset(\n",
    "        \"json\", data_files=f\"{DATA_PATH}/mixtral-8x7b-v1.json\", split=\"train\"\n",
    "    ).rename_column(\"labels\", \"provided_labels\")\n",
    "    # 識別のためのflagを追加\n",
    "    .map(lambda example: {\"flag\": \"mixtral\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.683458Z",
     "iopub.status.busy": "2024-03-20T06:11:11.682817Z",
     "iopub.status.idle": "2024-03-20T06:11:11.687969Z",
     "shell.execute_reply": "2024-03-20T06:11:11.687435Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"train_dataset\")\n",
    "print(train_dataset)\n",
    "\n",
    "# print(\"external_pii_dataset\")\n",
    "# print(external_pii_dataset)\n",
    "\n",
    "print(\"moredata_pii_dataset\")\n",
    "print(moredata_pii_dataset)\n",
    "\n",
    "print(\"mixtral\")\n",
    "print(mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.690126Z",
     "iopub.status.busy": "2024-03-20T06:11:11.689906Z",
     "iopub.status.idle": "2024-03-20T06:11:11.696470Z",
     "shell.execute_reply": "2024-03-20T06:11:11.695848Z"
    }
   },
   "outputs": [],
   "source": [
    "# 外部データと結合\n",
    "train_dataset = concatenate_datasets([train_dataset, mixtral, moredata_pii_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.699390Z",
     "iopub.status.busy": "2024-03-20T06:11:11.698701Z",
     "iopub.status.idle": "2024-03-20T06:11:11.703142Z",
     "shell.execute_reply": "2024-03-20T06:11:11.702530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"all train\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.705552Z",
     "iopub.status.busy": "2024-03-20T06:11:11.705092Z",
     "iopub.status.idle": "2024-03-20T06:11:11.710006Z",
     "shell.execute_reply": "2024-03-20T06:11:11.709481Z"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.712503Z",
     "iopub.status.busy": "2024-03-20T06:11:11.712080Z",
     "iopub.status.idle": "2024-03-20T06:11:11.715024Z",
     "shell.execute_reply": "2024-03-20T06:11:11.714539Z"
    }
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "if DEBUG:\n",
    "    train_dataset = train_dataset.select(range(300))\n",
    "    EPOCH = 1\n",
    "    print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:11.717258Z",
     "iopub.status.busy": "2024-03-20T06:11:11.716813Z",
     "iopub.status.idle": "2024-03-20T06:11:12.647704Z",
     "shell.execute_reply": "2024-03-20T06:11:12.647176Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))  # \\nを一つのtokenとして扱う\n",
    "# （これをしない場合、\\nは文字の先頭の_として扱われている。\n",
    "# 前: \\nSaito → \"_Saito\"\n",
    "# 後: \\nSaito → \"\\n\", \"Saito\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.649763Z",
     "iopub.status.busy": "2024-03-20T06:11:12.649559Z",
     "iopub.status.idle": "2024-03-20T06:11:12.653259Z",
     "shell.execute_reply": "2024-03-20T06:11:12.652804Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# labelを変換する\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-EMAIL\",\n",
    "    4: \"I-EMAIL\",\n",
    "    5: \"B-USERNAME\",\n",
    "    6: \"I-USERNAME\",\n",
    "    7: \"B-ID_NUM\",\n",
    "    8: \"I-ID_NUM\",\n",
    "    9: \"B-PHONE_NUM\",\n",
    "    10: \"I-PHONE_NUM\",\n",
    "    11: \"B-URL_PERSONAL\",\n",
    "    12: \"I-URL_PERSONAL\",\n",
    "    13: \"B-STREET_ADDRESS\",\n",
    "    14: \"I-STREET_ADDRESS\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.655096Z",
     "iopub.status.busy": "2024-03-20T06:11:12.654930Z",
     "iopub.status.idle": "2024-03-20T06:11:12.683798Z",
     "shell.execute_reply": "2024-03-20T06:11:12.683274Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    与えられたtokenとlabelから、\n",
    "    今回のtokenizerで区切った場合のtokenとlabelを作成する。\n",
    "    \"\"\"\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        # 文字数分だけ、該当のラベルを追加する\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "        # text -> ['Design', ' ']\n",
    "        # labels -> ['O', 'O', 'O', 'O', 'O', 'O', 'O'] (6文字分 + 空白1文字分)\n",
    "\n",
    "    # actual tokenization\n",
    "    # tokenized = tokenizer(\n",
    "    #     \"\".join(text),\n",
    "    #     return_offsets_mapping=True,\n",
    "    #     max_length=max_length,\n",
    "    #     truncation=True,\n",
    "    # )\n",
    "    tokenized = tokenizer(\n",
    "        \"\".join(text),\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=False,\n",
    "    )\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # offset_mappingの各エントリは、トークンが元のテキストのどの範囲（開始位置と終了位置）にマッピングされるかを示す\n",
    "        # タプルまたはリストで構成されます。\n",
    "\n",
    "        # CLS tokenの対応\n",
    "        # CLSやSEPには必ず(start_idx, end_idx) = (0, 0)が割り当てられる\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # 空白が存在する時は、offset_mappingのstart_idxを+1する\n",
    "        # DeBERTaV2Tokenizerは、空白・改行( と\\n)を文字の先頭に▁としてくっつけるため。\n",
    "        # NOTE: もし空白を▁として文字の先頭にくっつけるないtokenizerの場合は、不要\n",
    "        # if text[start_idx].isspace():\n",
    "        #     start_idx += 1\n",
    "        # \\nはisspace()に該当する。\n",
    "        # special tokenとして扱う場合はstart_idx += 1しないようにするため。\n",
    "        if text[start_idx] == \" \":\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    # Q: token_labelsは何の長さ？\n",
    "    # A: 今回のtokenizerで区切った時の、tokenに該当するlabel\n",
    "    # 例:\n",
    "    # 与えられたtoken example[\"tokens\"][:10]\n",
    "    # -> ['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n",
    "    # 今回のtokenizerで区切ったtoken　tokenizer.convert_ids_to_tokens(tokenized.input_ids[:10])\n",
    "    # -> ['[CLS]', '▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion', '-', 'Av', 'ril']\n",
    "    # 文字数が違う！！！\n",
    "    # 最初に与えられたtokenとそのlabelだと、今回のtokenizerで区切った場合のラベルが分からない。\n",
    "    # そのため、今回のtokenizerで区切った場合のtokenとラベルを作成した。\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
    "    num_proc=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.686033Z",
     "iopub.status.busy": "2024-03-20T06:11:12.685850Z",
     "iopub.status.idle": "2024-03-20T06:11:12.688320Z",
     "shell.execute_reply": "2024-03-20T06:11:12.687851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check aligh token label\n",
    "# for input_id, label in zip(train_dataset[\"input_ids\"][0], train_dataset[\"labels\"][0]):\n",
    "#     print(str(tokenizer.convert_ids_to_tokens(input_id)), id2label[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.690052Z",
     "iopub.status.busy": "2024-03-20T06:11:12.689872Z",
     "iopub.status.idle": "2024-03-20T06:11:12.693649Z",
     "shell.execute_reply": "2024-03-20T06:11:12.693193Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-EMAIL',\n",
       " 'I-EMAIL',\n",
       " 'B-USERNAME',\n",
       " 'I-USERNAME',\n",
       " 'B-ID_NUM',\n",
       " 'I-ID_NUM',\n",
       " 'B-PHONE_NUM',\n",
       " 'I-PHONE_NUM',\n",
       " 'B-URL_PERSONAL',\n",
       " 'I-URL_PERSONAL',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'I-STREET_ADDRESS']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id.keys())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.695429Z",
     "iopub.status.busy": "2024-03-20T06:11:12.695265Z",
     "iopub.status.idle": "2024-03-20T06:11:12.698637Z",
     "shell.execute_reply": "2024-03-20T06:11:12.698185Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.700360Z",
     "iopub.status.busy": "2024-03-20T06:11:12.700182Z",
     "iopub.status.idle": "2024-03-20T06:11:12.702917Z",
     "shell.execute_reply": "2024-03-20T06:11:12.702485Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad_to_multiple_of\n",
    "# paddingの際に、指定した数の倍数になるように、各サンプルの長さを揃える\n",
    "# ハードウェアの要件に合致することで、計算効率が良くなる可能性がある\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:12.704583Z",
     "iopub.status.busy": "2024-03-20T06:11:12.704415Z",
     "iopub.status.idle": "2024-03-20T06:11:13.215177Z",
     "shell.execute_reply": "2024-03-20T06:11:13.214532Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:13.218302Z",
     "iopub.status.busy": "2024-03-20T06:11:13.218048Z",
     "iopub.status.idle": "2024-03-20T06:11:13.225476Z",
     "shell.execute_reply": "2024-03-20T06:11:13.224459Z"
    }
   },
   "outputs": [],
   "source": [
    "def f5_score(precision: float, recall: float, beta: int = 5):\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    DeBERTa Tokenizerでの区切りを用いた評価値\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "\n",
    "    # f5scoreを追加\n",
    "    final_results[\"f5score\"] = f5_score(\n",
    "        results[\"overall_precision\"], results[\"overall_recall\"]\n",
    "    )\n",
    "\n",
    "    return final_results\n",
    "\n",
    "    # # seqevalのmetrics関数を使用して、精度、再現率、F1スコア、正解率を計算\n",
    "    # precision = results[\"overall_precision\"]\n",
    "    # recall = results[\"overall_recall\"]\n",
    "\n",
    "    # return {\n",
    "    #     \"precision\": precision,\n",
    "    #     \"recall\": recall,\n",
    "    #     \"f1\": results[\"overall_f1\"],\n",
    "    #     \"accuracy\": results[\"overall_accuracy\"],\n",
    "    #     \"f5score\": f5_score(precision, recall),\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:13.228630Z",
     "iopub.status.busy": "2024-03-20T06:11:13.228378Z",
     "iopub.status.idle": "2024-03-20T06:11:16.020576Z",
     "shell.execute_reply": "2024-03-20T06:11:16.019825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128016, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:16.023378Z",
     "iopub.status.busy": "2024-03-20T06:11:16.022819Z",
     "iopub.status.idle": "2024-03-20T06:11:16.026787Z",
     "shell.execute_reply": "2024-03-20T06:11:16.026265Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'document', 'provided_labels', 'trailing_whitespace', 'full_text', 'flag', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],\n",
       "    num_rows: 11162\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:16.029265Z",
     "iopub.status.busy": "2024-03-20T06:11:16.028819Z",
     "iopub.status.idle": "2024-03-20T06:11:16.032599Z",
     "shell.execute_reply": "2024-03-20T06:11:16.032112Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# with open(f\"{DATA_PATH}/document_pattern_dict.json\") as f:\n",
    "#     document_pattern_dict = json.load(f)\n",
    "\n",
    "\n",
    "# def document_id_to_pattern(example) -> str:\n",
    "#     return document_pattern_dict[example[\"document\"]]\n",
    "\n",
    "\n",
    "# def add_pattern_column(dataset):\n",
    "#     # original_datasetに対し、patternを付与する\n",
    "#     dataset = dataset.map(\n",
    "#         lambda example: {\"pattern\": document_id_to_pattern(example)}, num_proc=3\n",
    "#     )\n",
    "#     unique_labels = np.unique(dataset[\"pattern\"])\n",
    "\n",
    "#     # train_test_splitのstratifyに使うために、ClassLabelに変換する\n",
    "#     class_label_feature = ClassLabel(names=unique_labels.tolist())\n",
    "#     return dataset.cast_column(\"pattern\", class_label_feature)\n",
    "\n",
    "\n",
    "# def train_valid_split(train_dataset):\n",
    "#     # 'flag'列が'original'のもののみをテストデータとする\n",
    "#     original_dataset = train_dataset.filter(\n",
    "#         lambda x: x[\"flag\"] == \"original\", num_proc=3\n",
    "#     )\n",
    "#     extrenal_dataset = train_dataset.filter(\n",
    "#         lambda x: x[\"flag\"] != \"original\", num_proc=3\n",
    "#     )\n",
    "\n",
    "#     # pattern列を付与する\n",
    "#     original_dataset = add_pattern_column(original_dataset)\n",
    "\n",
    "#     # pattern列に対してstratifyになるよう分割する\n",
    "#     train_split_dataset = original_dataset.train_test_split(\n",
    "#         test_size=VALID_DATA_SIZE, seed=42, stratify_by_column=\"pattern\"\n",
    "#     )\n",
    "\n",
    "#     # trainについて、全てOのデータを除外する\n",
    "#     # train_split_dataset[\"train\"] = train_split_dataset[\"train\"].filter(\n",
    "#     #     lambda x: x[\"pattern\"] != 0, num_proc=3\n",
    "#     # )\n",
    "\n",
    "#     # validについて、全てOのデータを除外する\n",
    "#     # train_split_dataset[\"test\"] = train_split_dataset[\"test\"].filter(\n",
    "#     #     lambda x: x[\"pattern\"] != 0, num_proc=3\n",
    "#     # )\n",
    "\n",
    "#     # 'flag'列が'original'でないものを訓練データと検証データに分割する\n",
    "#     # train_split_dataset = original_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "#     # 再結合する\n",
    "#     concat_train_dataset = concatenate_datasets(\n",
    "#         [train_split_dataset[\"train\"], extrenal_dataset]\n",
    "#     )\n",
    "\n",
    "#     train_valid_dataset = DatasetDict(\n",
    "#         {\"train\": concat_train_dataset, \"valid\": train_split_dataset[\"test\"]}\n",
    "#     )\n",
    "#     return train_valid_dataset\n",
    "\n",
    "\n",
    "# if not DEBUG:\n",
    "#     train_valid_dataset = train_valid_split(train_dataset)\n",
    "# else:\n",
    "#     original_dataset = train_dataset.filter(lambda x: x[\"flag\"] == \"original\")\n",
    "#     extrenal_dataset = train_dataset.filter(lambda x: x[\"flag\"] != \"original\")\n",
    "\n",
    "#     train_split_dataset = original_dataset.train_test_split(\n",
    "#         test_size=VALID_DATA_SIZE, seed=42\n",
    "#     )\n",
    "\n",
    "#     concat_train_dataset = concatenate_datasets(\n",
    "#         [train_split_dataset[\"train\"], extrenal_dataset]\n",
    "#     )\n",
    "#     train_valid_dataset = DatasetDict(\n",
    "#         {\"train\": concat_train_dataset, \"valid\": train_split_dataset[\"test\"]}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:16.034842Z",
     "iopub.status.busy": "2024-03-20T06:11:16.034419Z",
     "iopub.status.idle": "2024-03-20T06:11:16.037088Z",
     "shell.execute_reply": "2024-03-20T06:11:16.036545Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:16.039431Z",
     "iopub.status.busy": "2024-03-20T06:11:16.038905Z",
     "iopub.status.idle": "2024-03-20T06:11:18.139431Z",
     "shell.execute_reply": "2024-03-20T06:11:18.138725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LOG_PATH,\n",
    "    learning_rate=1e-5,  # 2e-5,\n",
    "    per_device_train_batch_size=2,  # 16,  # 32はだめ、性能がepoch0の時、precision0.31→0.00、Recall0.25→0.00に落ちる\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=2,  # 16,  # 32,↑同様、バッチサイズが非常に重要なパラメーターであるとも言える\n",
    "    num_train_epochs=5,  # 3,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    do_eval=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f5score\",  # add\n",
    "    greater_is_better=True,  # add\n",
    "    warmup_ratio=0.1,  # add\n",
    "    lr_scheduler_type=\"cosine\",  # add\n",
    "    report_to=REPORT_TO,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # train_dataset=train_valid_dataset[\"train\"],\n",
    "    # eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T06:11:18.142524Z",
     "iopub.status.busy": "2024-03-20T06:11:18.141897Z",
     "iopub.status.idle": "2024-03-20T06:18:27.417642Z",
     "shell.execute_reply": "2024-03-20T06:18:27.416641Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='13950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  543/13950 07:05 < 2:55:36, 1.27 it/s, Epoch 0.19/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 968.00 MiB (GPU 0; 47.54 GiB total capacity; 44.74 GiB already allocated; 377.12 MiB free; 45.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# モデルの学習\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     cv_score \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f5score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# モデルの保存\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:2772\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2772\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2775\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:2795\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2794\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/amp/autocast_mode.py:12\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1404\u001b[0m, in \u001b[0;36mDebertaV2ForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1417\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1070\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1062\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1063\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1064\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1068\u001b[0m )\n\u001b[0;32m-> 1070\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:514\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    504\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    505\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    506\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output_attentions,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    524\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:734\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# bsz x height x length x dimension\u001b[39;00m\n\u001b[1;32m    733\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m XSoftmax\u001b[38;5;241m.\u001b[39mapply(attention_scores, attention_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 734\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(\n\u001b[1;32m    736\u001b[0m     attention_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), value_layer\n\u001b[1;32m    737\u001b[0m )\n\u001b[1;32m    738\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    739\u001b[0m     context_layer\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, context_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), context_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    742\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:234\u001b[0m, in \u001b[0;36mStableDropout.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mCall the module\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    x (`torch.tensor`): The input tensor to apply dropout\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXDropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:177\u001b[0m, in \u001b[0;36mXDropout.forward\u001b[0;34m(ctx, input, local_ctx)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m, local_ctx):\n\u001b[0;32m--> 177\u001b[0m     mask, dropout \u001b[38;5;241m=\u001b[39m \u001b[43mget_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:162\u001b[0m, in \u001b[0;36mget_mask\u001b[0;34m(input, local_context)\u001b[0m\n\u001b[1;32m    159\u001b[0m     mask \u001b[38;5;241m=\u001b[39m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mreuse_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbernoulli_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(local_context, DropoutContext):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 968.00 MiB (GPU 0; 47.54 GiB total capacity; 44.74 GiB already allocated; 377.12 MiB free; 45.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # モデルの学習\n",
    "    trainer.train()\n",
    "    cv_score = trainer.evaluate()[\"eval_f5score\"]\n",
    "    # モデルの保存\n",
    "    trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     \".\",\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CV DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_valid_preds(trainer, valid_dataset):\n",
    "#     \"\"\"\n",
    "#     trainerを用いてvalid_datasetに対する予測を行う\n",
    "#     \"\"\"\n",
    "#     predictions = trainer.predict(valid_dataset).predictions\n",
    "#     preds_final = predictions.argmax(-1)\n",
    "\n",
    "#     return preds_final\n",
    "\n",
    "\n",
    "# # def get_valid_preds_with_pp(trainer: Trainer, valid_dataset, threhosld: float):\n",
    "# #     \"\"\"\n",
    "# #     trainerを用いてvalid_datasetに対する予測を行う\n",
    "# #     \"\"\"\n",
    "# #     predictions = trainer.predict(valid_dataset).predictions\n",
    "# #     pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n",
    "# #         predictions.shape[0], predictions.shape[1], 1\n",
    "# #     )\n",
    "\n",
    "# #     preds = predictions.argmax(-1)\n",
    "# #     preds_without_O = pred_softmax[:, :, 1:].argmax(-1) + 1\n",
    "# #     O_preds = pred_softmax[:, :, 0]\n",
    "\n",
    "# #     preds_final = np.where(O_preds < threhosld, preds_without_O, preds)\n",
    "\n",
    "# #     return preds_final\n",
    "\n",
    "\n",
    "# # def get_valid_preds_with_pp_for_opt(trainer: Trainer, valid_dataset):\n",
    "# #     \"\"\"\n",
    "# #     trainerを用いてvalid_datasetに対する予測を行う\n",
    "# #     \"\"\"\n",
    "# #     predictions = trainer.predict(valid_dataset).predictions\n",
    "# #     pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n",
    "# #         predictions.shape[0], predictions.shape[1], 1\n",
    "# #     )\n",
    "\n",
    "# #     preds = predictions.argmax(-1)\n",
    "# #     preds_without_O = pred_softmax[:, :, 1:].argmax(-1) + 1\n",
    "# #     O_preds = pred_softmax[:, :, 0]\n",
    "\n",
    "# #     return O_preds, preds_without_O, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tokenize(example, tokenizer):\n",
    "#     text = []\n",
    "#     token_map = []\n",
    "\n",
    "#     idx = 0\n",
    "\n",
    "#     for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "#         text.append(t)\n",
    "#         token_map.extend([idx] * len(t))\n",
    "#         if ws:\n",
    "#             text.append(\" \")\n",
    "#             token_map.append(-1)\n",
    "\n",
    "#         idx += 1\n",
    "\n",
    "#     # tokenized = tokenizer(\n",
    "#     #     \"\".join(text),\n",
    "#     #     return_offsets_mapping=True,\n",
    "#     #     truncation=True,\n",
    "#     #     max_length=INFERENCE_MAX_LENGTH,\n",
    "#     # )\n",
    "\n",
    "#     tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)\n",
    "\n",
    "#     return {\n",
    "#         **tokenized,\n",
    "#         \"token_map\": token_map,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_output_part(preds_final, valid_dataset):\n",
    "#     # triplets = []\n",
    "#     document, token, label, token_str = [], [], [], []\n",
    "#     # token_mapはoffsetsの文字列indexが、何番目のtokenの紐付け\n",
    "#     # start_idx = 0の時、tokens[token_map[start_idx]] → 'Design'\n",
    "#     # start_idx = 1の時、tokens[token_map[start_idx]] → 'Design'\n",
    "#     # ・・・\n",
    "#     # start_idx = 5の時、tokens[token_map[start_idx]] → 'Design'\n",
    "#     # start_idx = 6の時、tokens[token_map[start_idx]] → '\\n\\n'\n",
    "\n",
    "#     # 同じtoken_idに、二つの予測結果が入ってしまう問題\n",
    "#     # 原因は、「文字列をtokenizerで区切り、別のlabelと予測した場合を、別のtripletとして扱うから」\n",
    "#     # 例えば、'kellyharrison@gmail.com'のlabelの予測結果\n",
    "#     # '\\nkelly' -> 'B-EMAIL'\n",
    "#     # '##harris' -> 'B-EMAIL'\n",
    "#     # '##on' -> 'I-NAME_STUDENT'\n",
    "#     # '@' -> 'B-EMAIL'\n",
    "#     # 'gmail' -> 'B-EMAIL'\n",
    "#     # '.' -> 'B-EMAIL'\n",
    "#     # 'com' -> 'B-EMAIL'\n",
    "#     # 解決策案: 同じtoken_idの場合は、tripletに追加しない\n",
    "\n",
    "#     for p, token_map, offsets, tokens, doc in zip(\n",
    "#         preds_final,\n",
    "#         valid_dataset[\"token_map\"],\n",
    "#         valid_dataset[\"offset_mapping\"],\n",
    "#         valid_dataset[\"tokens\"],\n",
    "#         valid_dataset[\"document\"],\n",
    "#     ):\n",
    "#         triplets = []\n",
    "#         for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "#             label_pred = id2label[token_pred]\n",
    "\n",
    "#             if start_idx + end_idx == 0:\n",
    "#                 continue\n",
    "\n",
    "#             if token_map[start_idx] == -1:\n",
    "#                 start_idx += 1\n",
    "\n",
    "#             # ignore \"\\n\\n\"\n",
    "#             # TODO: 答えに\\nが入っている場合がありそうだけど、この処理は本当に問題ない？\n",
    "#             # while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "#             #     start_idx += 1\n",
    "\n",
    "#             # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "#             # while start_idx < len(token_map) and tokens[token_map[start_idx]] == \" \":\n",
    "#             #     start_idx += 1\n",
    "\n",
    "#             # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "#             while (\n",
    "#                 start_idx < len(token_map)\n",
    "#                 and tokens[token_map[start_idx]].isspace()\n",
    "#                 and tokens[token_map[start_idx]] != \"\\n\"\n",
    "#             ):\n",
    "#                 start_idx += 1\n",
    "\n",
    "#             if start_idx >= len(token_map):\n",
    "#                 break\n",
    "\n",
    "#             token_id = token_map[start_idx]\n",
    "\n",
    "#             # ignore \"O\" predictions and whitespace preds\n",
    "#             # if label_pred != \"O\" and token_id != -1:\n",
    "#             # \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\"についてはpostprocessを行う\n",
    "#             if (\n",
    "#                 label_pred\n",
    "#                 not in (\n",
    "#                     \"O\",\n",
    "#                     \"B-EMAIL\",\n",
    "#                     \"B-PHONE_NUM\",\n",
    "#                     \"I-PHONE_NUM\",\n",
    "#                     \"B-URL_PERSONAL\",\n",
    "#                     \"I-URL_PERSONAL\",\n",
    "#                 )\n",
    "#                 and token_id != -1\n",
    "#             ):\n",
    "#                 triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "#                 if triplet not in triplets:\n",
    "#                     if (\n",
    "#                         len(triplets) >= 1\n",
    "#                         and document[-1] == doc\n",
    "#                         and token[-1] == token_id\n",
    "#                     ):\n",
    "#                         continue\n",
    "#                     document.append(doc)\n",
    "#                     token.append(token_id)\n",
    "#                     label.append(label_pred)\n",
    "#                     token_str.append(tokens[token_id])\n",
    "#                     triplets.append(triplet)\n",
    "\n",
    "#     return document, token, label, token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "\n",
    "# nlp = English()\n",
    "\n",
    "\n",
    "# def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "#     idx = 0\n",
    "#     spans = []\n",
    "#     span = []\n",
    "\n",
    "#     for i, token in enumerate(document):\n",
    "#         if token != target[idx]:\n",
    "#             idx = 0\n",
    "#             span = []\n",
    "#             continue\n",
    "#         span.append(i)\n",
    "#         idx += 1\n",
    "#         if idx == len(target):\n",
    "#             spans.append(span)\n",
    "#             span = []\n",
    "#             idx = 0\n",
    "#             continue\n",
    "\n",
    "#     return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# valid_dataset = train_dataset.filter(\n",
    "#     lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "#     num_proc=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from typing import Optional\n",
    "\n",
    "\n",
    "# def get_rulebase(regex: re.Pattern, data, name: str) -> Optional[list[dict]]:\n",
    "#     output = []\n",
    "#     matches = regex.findall(data[\"full_text\"])\n",
    "\n",
    "#     # NOTE: find_spanにおいて、同じ単語を全て見つけてしまうため、重複をなくす\n",
    "#     matches = list(dict.fromkeys(matches))\n",
    "\n",
    "#     if not matches:\n",
    "#         return None\n",
    "\n",
    "#     matched_spans = []\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans.append(find_span(target, data[\"tokens\"]))\n",
    "\n",
    "#     for matched_span in matched_spans:\n",
    "#         for one_token_span in matched_span:\n",
    "#             for intermediate, token_idx in enumerate(one_token_span):\n",
    "#                 if intermediate == 0:\n",
    "#                     prefix = \"B\"\n",
    "#                 else:\n",
    "#                     prefix = \"I\"\n",
    "\n",
    "#                 output.append(\n",
    "#                     {\n",
    "#                         \"document\": data[\"document\"],\n",
    "#                         \"token\": token_idx,\n",
    "#                         \"label\": f\"{prefix}-{name}\",\n",
    "#                         \"token_str\": data[\"tokens\"][token_idx],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# email_regex = re.compile(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\")\n",
    "# phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "# url_regex = re.compile(\n",
    "#     r\"https?://(?:www\\.)?[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+){1,2}/(?:[a-zA-Z0-9-]+/)*(?:[a-zA-Z0-9-]+\\.html|.*\\.php|.*\\.asp|.*\\.jsp|\\?v=[a-zA-Z0-9-_]+|.*\\.htm|user/[a-zA-Z0-9-_]+|watch\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|)?\"\n",
    "# )\n",
    "\n",
    "# emails_tst = []\n",
    "# emails = []\n",
    "# phone_nums = []\n",
    "# urls = []\n",
    "\n",
    "# for _data in valid_dataset:\n",
    "#     # email\n",
    "#     if match := get_rulebase(email_regex, _data, \"EMAIL\"):\n",
    "#         emails.extend(match)\n",
    "\n",
    "#     # phone number\n",
    "#     if match := get_rulebase(phone_num_regex, _data, \"PHONE_NUM\"):\n",
    "#         phone_nums.extend(match)\n",
    "\n",
    "#     # URL\n",
    "#     if match := get_rulebase(url_regex, _data, \"URL_PERSONAL\"):\n",
    "#         urls.extend(match)\n",
    "\n",
    "# pp_data = [emails, phone_nums, urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def make_correct_df(train: pl.DataFrame):\n",
    "#     # 学習データから、outputと同様のデータフレームを作成する\n",
    "#     outputs = []\n",
    "#     for document_id, token, label in zip(\n",
    "#         train[\"document\"], train[\"tokens\"], train[\"provided_labels\"]\n",
    "#     ):\n",
    "#         for token, (token_str, label_one) in enumerate(zip(token, label)):\n",
    "#             if label_one != \"O\":\n",
    "#                 outputs.append((document_id, token, label_one, token_str))\n",
    "#     return pl.DataFrame(outputs, schema=[\"document\", \"token\", \"label\", \"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def make_correct_pred_join_df(\n",
    "#     train_correct_df: pl.DataFrame, valid_pred_df: pl.DataFrame\n",
    "# ) -> pl.DataFrame:\n",
    "#     \"\"\"\n",
    "#     validで利用したdocumentのみを抽出し、train_correct_dfとvalid_pred_dfを結合して、documentごとに比較できるようにする\n",
    "#     \"\"\"\n",
    "#     out = train_correct_df.filter(\n",
    "#         pl.col(\"document\").is_in(valid_pred_df[\"document\"])\n",
    "#     ).join(valid_pred_df, on=[\"document\", \"token\"], how=\"outer\", suffix=\"_pred\")\n",
    "\n",
    "#     joined_dfs = []\n",
    "#     for document in out[\"document\"].unique().to_list():\n",
    "#         if document is None:\n",
    "#             continue\n",
    "#         joined_df_per_document = out.filter(\n",
    "#             (pl.col(\"document\") == document) | (pl.col(\"document_pred\") == document)\n",
    "#         )\n",
    "#         joined_dfs.append(joined_df_per_document)\n",
    "\n",
    "#     return pl.concat(joined_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # main\n",
    "# valid_dataset = train_dataset.filter(\n",
    "#     lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "#     num_proc=3,\n",
    "# )\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    "# )\n",
    "\n",
    "# # 閾値を緩める後処理を使わない場合\n",
    "# valid_preds = get_valid_preds(trainer, valid_dataset)\n",
    "# # 閾値を緩める後処理を使う場合\n",
    "# # valid_preds = get_valid_preds_with_pp(trainer, valid_dataset, threhosld=0.90)\n",
    "\n",
    "# document, token, label, token_str = get_output_part(valid_preds, valid_dataset)\n",
    "\n",
    "# valid_pred_df = pl.DataFrame(\n",
    "#     [document, token, label, token_str],\n",
    "#     schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    "# )\n",
    "\n",
    "# postprocess_df = pl.concat([pl.DataFrame(val) for val in pp_data if val != []])\n",
    "# valid_pred_df = pl.concat([valid_pred_df, postprocess_df])\n",
    "\n",
    "# train_correct_df = make_correct_df(pl.from_pandas(train_dataset.to_pandas()))\n",
    "# # train_correct_df = make_correct_df(train)\n",
    "\n",
    "# valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         EMAIL       0.94      1.00      0.97        17\n",
      "        ID_NUM       0.67      0.94      0.78        31\n",
      "  NAME_STUDENT       0.87      0.98      0.92       680\n",
      "     PHONE_NUM       1.00      1.00      1.00         4\n",
      "STREET_ADDRESS       1.00      1.00      1.00         2\n",
      "  URL_PERSONAL       0.64      0.93      0.76        57\n",
      "      USERNAME       0.25      1.00      0.40         2\n",
      "\n",
      "     micro avg       0.83      0.97      0.90       793\n",
      "     macro avg       0.77      0.98      0.83       793\n",
      "  weighted avg       0.84      0.97      0.90       793\n",
      "\n",
      "classification_report\n",
      "{'EMAIL': {'precision': 0.9444444444444444, 'recall': 1.0, 'f1-score': 0.9714285714285714, 'support': 17}, 'ID_NUM': {'precision': 0.6744186046511628, 'recall': 0.9354838709677419, 'f1-score': 0.7837837837837838, 'support': 31}, 'NAME_STUDENT': {'precision': 0.8670143415906127, 'recall': 0.9779411764705882, 'f1-score': 0.9191430545957152, 'support': 680}, 'PHONE_NUM': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'STREET_ADDRESS': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'URL_PERSONAL': {'precision': 0.6385542168674698, 'recall': 0.9298245614035088, 'f1-score': 0.757142857142857, 'support': 57}, 'USERNAME': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 2}, 'micro avg': {'precision': 0.8345945945945946, 'recall': 0.9735182849936949, 'f1-score': 0.8987194412107102, 'support': 793}, 'macro avg': {'precision': 0.7677759439362414, 'recall': 0.9776070869774056, 'f1-score': 0.8330711809929897, 'support': 793}, 'weighted avg': {'precision': 0.8441738649972307, 'recall': 0.9735182849936949, 'f1-score': 0.9026305207992588, 'support': 793}}\n",
      "f5score_from_valid_df\n",
      "0.9673253012048194\n"
     ]
    }
   ],
   "source": [
    "# from seqeval.metrics import classification_report\n",
    "\n",
    "\n",
    "# def calc_f5_score_from_valid_df(\n",
    "#     valid_correct_pred_df: pl.DataFrame, train: pl.DataFrame\n",
    "# ) -> tuple[dict, float]:\n",
    "#     # trainのtokenの長さを追加する\n",
    "#     pred_df_agg_with_len = (\n",
    "#         valid_correct_pred_df.select(\n",
    "#             pl.col(\"document_pred\").cast(pl.Int64),\n",
    "#             pl.col(\"token_pred\").cast(pl.Int64),\n",
    "#             pl.col(\"label_pred\"),\n",
    "#         )\n",
    "#         .drop_nulls()\n",
    "#         .sort(\"document_pred\")\n",
    "#         .group_by(\"document_pred\")\n",
    "#         .agg(\n",
    "#             pl.col(\"token_pred\"),\n",
    "#             pl.col(\"label_pred\"),\n",
    "#         )\n",
    "#         .join(\n",
    "#             train.with_columns(\n",
    "#                 pl.col(\"tokens\").map_elements(len).alias(\"tokens_len\"),\n",
    "#             ).select([\"document\", \"tokens_len\", \"labels\"]),\n",
    "#             left_on=\"document_pred\",\n",
    "#             right_on=\"document\",\n",
    "#             how=\"left\",\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # 推論したlabel列をOを含むtoken列へと変換する\n",
    "#     label_pred_alls = []\n",
    "#     for token_pred, label_pred, tokens_len in zip(\n",
    "#         pred_df_agg_with_len[\"token_pred\"],\n",
    "#         pred_df_agg_with_len[\"label_pred\"],\n",
    "#         pred_df_agg_with_len[\"tokens_len\"],\n",
    "#     ):\n",
    "#         label_pred_all = [\"O\" for _ in range(tokens_len)]\n",
    "#         for token, label in zip(token_pred, label_pred):\n",
    "#             label_pred_all[token] = label\n",
    "#         label_pred_alls.append(label_pred_all)\n",
    "\n",
    "#     actual_pred_df = pred_df_agg_with_len.with_columns(\n",
    "#         pl.Series(\"label_pred_all\", label_pred_alls)\n",
    "#     ).select([\"labels\", \"label_pred_all\"])\n",
    "\n",
    "#     # ログ用\n",
    "#     print(\"classification report\")\n",
    "#     print(\n",
    "#         classification_report(\n",
    "#             actual_pred_df[\"labels\"].to_list(),\n",
    "#             actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     cls_rep_dict = classification_report(\n",
    "#         actual_pred_df[\"labels\"].to_list(),\n",
    "#         actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "#         output_dict=True,\n",
    "#     )\n",
    "\n",
    "#     f5score_from_valid_df = precision_recall_fscore_support(\n",
    "#         actual_pred_df[\"labels\"].to_list(),\n",
    "#         actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "#         beta=5,\n",
    "#         average=\"micro\",\n",
    "#     )[2]\n",
    "\n",
    "#     print(\"classification_report\")\n",
    "#     print(cls_rep_dict)\n",
    "#     print(\"f5score_from_valid_df\")\n",
    "#     print(f5score_from_valid_df)\n",
    "\n",
    "#     return cls_rep_dict, f5score_from_valid_df\n",
    "\n",
    "\n",
    "# cls_rep_dict, f5score_from_valid_df = calc_f5_score_from_valid_df(\n",
    "#     valid_correct_pred_df, train\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postprocessの閾値の最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先に推論結果を取得する\n",
    "# valid_dataset = train_dataset.filter(\n",
    "#     lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "#     num_proc=3,\n",
    "# )\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    "# )\n",
    "\n",
    "# train_correct_df = make_correct_df(pl.from_pandas(train_dataset.to_pandas()))\n",
    "\n",
    "# O_preds, preds_without_O, preds = get_valid_preds_with_pp_for_opt(\n",
    "#     trainer, valid_dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.50から0.99まで、0.01刻みで、valid_predsを取得する\n",
    "# # TODO: 遅すぎる(10分ぐらいかかりそう)ので、実行を早くする\n",
    "# best_f5score_from_valid_df = 0\n",
    "# for thr in np.linspace(0.80, 1.00, 20):\n",
    "#     # 推論\n",
    "#     valid_preds = np.where(O_preds < thr, preds_without_O, preds)\n",
    "\n",
    "#     # valid_correct_pred_dfを作成\n",
    "#     document, token, label, token_str = get_output_part(\n",
    "#         valid_preds, valid_dataset\n",
    "#     )  # 遅い10秒くらい\n",
    "\n",
    "#     valid_pred_df = pl.DataFrame(\n",
    "#         [document, token, label, token_str],\n",
    "#         schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    "#     )\n",
    "\n",
    "#     postprocess_df = pl.concat(\n",
    "#         [pl.DataFrame(val) for val in [emails, phone_nums] if val != []]\n",
    "#     )\n",
    "#     valid_pred_df = pl.concat([valid_pred_df, postprocess_df])\n",
    "\n",
    "#     valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)\n",
    "\n",
    "#     cls_rep_dict, f5score_from_valid_df = calc_f5_score_from_valid_df(\n",
    "#         valid_correct_pred_df, train\n",
    "#     )  # 遅い10秒くらい\n",
    "#     print(thr, f5score_from_valid_df)\n",
    "\n",
    "#     if best_f5score_from_valid_df < f5score_from_valid_df:\n",
    "#         best_cls_rep_dict = cls_rep_dict\n",
    "#         best_f5score_from_valid_df = f5score_from_valid_df\n",
    "\n",
    "# print(best_cls_rep_dict, best_f5score_from_valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVをwandbにuploadする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not DEBUG:\n",
    "#     # valid_df\n",
    "#     tbl = wandb.Table(data=valid_correct_pred_df.to_pandas())\n",
    "#     wandb.log({\"valid_correct_pred_df\": tbl})\n",
    "\n",
    "#     # valid score\n",
    "#     wandb.run.summary[\"f5score_from_valid_df\"] = f5score_from_valid_df\n",
    "\n",
    "#     # classification report\n",
    "#     table = wandb.Table(columns=[\"Tag\", \"Precision\", \"Recall\", \"F1Score\", \"Support\"])\n",
    "#     for name in cls_rep_dict.keys():\n",
    "#         table.add_data(\n",
    "#             name,\n",
    "#             cls_rep_dict[name][\"precision\"],\n",
    "#             cls_rep_dict[name][\"recall\"],\n",
    "#             cls_rep_dict[name][\"f1-score\"],\n",
    "#             cls_rep_dict[name][\"support\"],\n",
    "#         )\n",
    "#     wandb.log({\"classification_report\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(\"cp /notebooks/pll_data_detection/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    if \"_\" in dataset_name:\n",
    "        raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "    dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "    dataset_metadata[\"title\"] = dataset_name\n",
    "    with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "\n",
    "if (not DEBUG) and UPLOAD_DATA:\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pll_data_detection/trained_models/e054-fine-tuning-all\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42d2658b8b948f29920bc8445613742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.282 MB of 0.282 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f5score_from_valid_df</td><td>0.96839</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">e052-add-url-pp</strong> at: <a href='https://wandb.ai/sinchir0/pll/runs/79i518fx' target=\"_blank\">https://wandb.ai/sinchir0/pll/runs/79i518fx</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240320_015928-79i518fx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not DEBUG:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "057f8381e1894a2eb0c122b247f1ee46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf7cfa33cc004e29894fcfc8cf7655b0",
       "placeholder": "​",
       "style": "IPY_MODEL_4f607cac3ed54191a58225d46208412a",
       "tabbable": null,
       "tooltip": null,
       "value": "Map (num_proc=3): 100%"
      }
     },
     "0d10d268ff7f4af0b8f9cc3b16725f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2371222ab9b94dc1903e80e48b5c67bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8d194ac65d534f7696b48336a3638722",
       "placeholder": "​",
       "style": "IPY_MODEL_dfa022d93d3f4efaad1a2083588b6d66",
       "tabbable": null,
       "tooltip": null,
       "value": "Filter (num_proc=3): 100%"
      }
     },
     "270aea56ef634fdbb8e65cd2632e4789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fbc2ce656fd24669bc27b24630d4a4ad",
       "placeholder": "​",
       "style": "IPY_MODEL_94412be9dd4844a5bf6362a3ffe38e09",
       "tabbable": null,
       "tooltip": null,
       "value": " 11162/11162 [00:23&lt;00:00, 614.02 examples/s]"
      }
     },
     "27efdfa384b84388b81368f507b1b33f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28b451f074f94f529cda44565d1a9fd5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f607cac3ed54191a58225d46208412a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "566c9b70bd6c408699cb46dc917c98c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_057f8381e1894a2eb0c122b247f1ee46",
        "IPY_MODEL_c4ce37dcbfe04ae3a85e139d6c84ad2d",
        "IPY_MODEL_ee87acd74065486581d29f0e29791b8c"
       ],
       "layout": "IPY_MODEL_795e0f2452d24336bef645dea2f75836",
       "tabbable": null,
       "tooltip": null
      }
     },
     "61c86debe4c94c95a1e0fb280f766502": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2371222ab9b94dc1903e80e48b5c67bc",
        "IPY_MODEL_8d7aa18a39e84ec88c9807478abfcc79",
        "IPY_MODEL_270aea56ef634fdbb8e65cd2632e4789"
       ],
       "layout": "IPY_MODEL_28b451f074f94f529cda44565d1a9fd5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "795e0f2452d24336bef645dea2f75836": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d194ac65d534f7696b48336a3638722": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d7aa18a39e84ec88c9807478abfcc79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1c63cd3e39346a7b5b9bb0ef988386f",
       "max": 11162,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bf676d98724f4a46b8e32aff4a947799",
       "tabbable": null,
       "tooltip": null,
       "value": 11162
      }
     },
     "94412be9dd4844a5bf6362a3ffe38e09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ba2cb3993bcb44c0abb21d9cac8bb3a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bf676d98724f4a46b8e32aff4a947799": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bf7cfa33cc004e29894fcfc8cf7655b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4ce37dcbfe04ae3a85e139d6c84ad2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27efdfa384b84388b81368f507b1b33f",
       "max": 473,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ba2cb3993bcb44c0abb21d9cac8bb3a8",
       "tabbable": null,
       "tooltip": null,
       "value": 473
      }
     },
     "d6110264c6f0475bb1d2dd040b0aa9d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dfa022d93d3f4efaad1a2083588b6d66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ee87acd74065486581d29f0e29791b8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0d10d268ff7f4af0b8f9cc3b16725f40",
       "placeholder": "​",
       "style": "IPY_MODEL_d6110264c6f0475bb1d2dd040b0aa9d6",
       "tabbable": null,
       "tooltip": null,
       "value": " 473/473 [00:01&lt;00:00, 610.89 examples/s]"
      }
     },
     "f1c63cd3e39346a7b5b9bb0ef988386f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbc2ce656fd24669bc27b24630d4a4ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
