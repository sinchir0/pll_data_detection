{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "- lr_scheduler_typeをcosineから、cosine_with_restartsに変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:04.761693Z",
     "iopub.status.busy": "2024-03-27T02:17:04.761501Z",
     "iopub.status.idle": "2024-03-27T02:17:04.771663Z",
     "shell.execute_reply": "2024-03-27T02:17:04.771013Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Kernel By VSCode or nohup!\n",
      "Jupyter Kernel By VSCode or nohup!\n",
      "Jupyter Kernel By VSCode or nohup!\n",
      "Jupyter Kernel By VSCode or nohup!\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"e075-sche-cos-re\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "DATA_PATH = \"pll_data_detection/data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "if len(DATASET_NAME) < 6 or len(DATASET_NAME) > 50:\n",
    "    raise Exception(f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\")\n",
    "LOG_PATH = f\"pll_data_detection/log/{EXP_NAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"pll_data_detection/trained_models/{EXP_NAME}\"\n",
    "\n",
    "DEBUG = False\n",
    "UPLOAD_DATA = True\n",
    "TRAINING = True\n",
    "# USE_TRAINED_MODEL_PATH = \"pll_data_detection/trained_models/e050-add-epoch\"\n",
    "\n",
    "VALID_DATA_SIZE = 0.5\n",
    "SEED = 42\n",
    "EPOCH = 3\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def resolve_path(base_path: str) -> str:\n",
    "    cwd = os.getcwd()\n",
    "    if cwd == \"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode or nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == \"/notebooks/pll_data_detection/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "\n",
    "# if not TRAINING:\n",
    "#     MODEL_OUTPUT_PATH = USE_TRAINED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:04.801922Z",
     "iopub.status.busy": "2024-03-27T02:17:04.801668Z",
     "iopub.status.idle": "2024-03-27T02:17:04.805463Z",
     "shell.execute_reply": "2024-03-27T02:17:04.804822Z"
    }
   },
   "outputs": [],
   "source": [
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:04.807817Z",
     "iopub.status.busy": "2024-03-27T02:17:04.807587Z",
     "iopub.status.idle": "2024-03-27T02:17:30.582873Z",
     "shell.execute_reply": "2024-03-27T02:17:30.581997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q polars==0.20.10\n",
    "%pip install -q transformers==4.37.2\n",
    "%pip install -q datasets==2.16.1\n",
    "%pip install -q evaluate==0.4.1\n",
    "%pip install -q seqeval==1.2.2\n",
    "%pip install -q accelerate\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q wandb==0.16.3\n",
    "\n",
    "# formatter\n",
    "%pip install -q black isort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:30.586369Z",
     "iopub.status.busy": "2024-03-27T02:17:30.585816Z",
     "iopub.status.idle": "2024-03-27T02:17:36.687952Z",
     "shell.execute_reply": "2024-03-27T02:17:36.687146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets, Value, ClassLabel\n",
    "from seqeval.metrics.sequence_labeling import precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:36.691650Z",
     "iopub.status.busy": "2024-03-27T02:17:36.691176Z",
     "iopub.status.idle": "2024-03-27T02:17:36.753824Z",
     "shell.execute_reply": "2024-03-27T02:17:36.753122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "assert transformers.__version__ == \"4.37.2\"\n",
    "assert datasets.__version__ == \"2.16.1\"\n",
    "assert evaluate.__version__ == \"0.4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:36.756927Z",
     "iopub.status.busy": "2024-03-27T02:17:36.756697Z",
     "iopub.status.idle": "2024-03-27T02:17:36.763510Z",
     "shell.execute_reply": "2024-03-27T02:17:36.762916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:36.766021Z",
     "iopub.status.busy": "2024-03-27T02:17:36.765816Z",
     "iopub.status.idle": "2024-03-27T02:17:38.299917Z",
     "shell.execute_reply": "2024-03-27T02:17:38.299159Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if not DEBUG:\n",
    "    load_dotenv(f\"{DATA_PATH}/.env\")\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"pll\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:38.302950Z",
     "iopub.status.busy": "2024-03-27T02:17:38.302428Z",
     "iopub.status.idle": "2024-03-27T02:17:39.094035Z",
     "shell.execute_reply": "2024-03-27T02:17:39.093137Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:39.097061Z",
     "iopub.status.busy": "2024-03-27T02:17:39.096824Z",
     "iopub.status.idle": "2024-03-27T02:17:40.122458Z",
     "shell.execute_reply": "2024-03-27T02:17:40.121548Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:40.125711Z",
     "iopub.status.busy": "2024-03-27T02:17:40.125478Z",
     "iopub.status.idle": "2024-03-27T02:17:40.341856Z",
     "shell.execute_reply": "2024-03-27T02:17:40.341240Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    load_dataset(\n",
    "        \"json\", data_files=f\"{DATA_PATH}/train.json\", split=\"train\"\n",
    "    ).rename_column(\"labels\", \"provided_labels\")\n",
    "    # documentをstringに変換\n",
    "    .cast_column(\"document\", Value(dtype=\"string\", id=None))\n",
    "    # 識別のためのflagを追加\n",
    "    .map(lambda example: {\"flag\": \"original\"})\n",
    ")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files={\"test\": f\"{DATA_PATH}/test.json\"}, split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:40.344882Z",
     "iopub.status.busy": "2024-03-27T02:17:40.344672Z",
     "iopub.status.idle": "2024-03-27T02:17:42.020064Z",
     "shell.execute_reply": "2024-03-27T02:17:42.019408Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document</th><th>full_text</th><th>tokens</th><th>trailing_whitespace</th><th>labels</th></tr><tr><td>i64</td><td>str</td><td>list[str]</td><td>list[bool]</td><td>list[str]</td></tr></thead><tbody><tr><td>7</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>10</td><td>&quot;Diego Estrada\n",
       "…</td><td>[&quot;Diego&quot;, &quot;Estrada&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;B-NAME_STUDENT&quot;, &quot;I-NAME_STUDENT&quot;, … &quot;O&quot;]</td></tr><tr><td>16</td><td>&quot;Reporting proc…</td><td>[&quot;Reporting&quot;, &quot;process&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>20</td><td>&quot;Design Thinkin…</td><td>[&quot;Design&quot;, &quot;Thinking&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[true, true, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr><tr><td>56</td><td>&quot;Assignment:  V…</td><td>[&quot;Assignment&quot;, &quot;:&quot;, … &quot;\n",
       "\n",
       "&quot;]</td><td>[false, false, … false]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬─────────────────────┬─────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ document ┆ full_text           ┆ tokens              ┆ trailing_whitespace ┆ labels              │\n",
       "│ ---      ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---                 │\n",
       "│ i64      ┆ str                 ┆ list[str]           ┆ list[bool]          ┆ list[str]           │\n",
       "╞══════════╪═════════════════════╪═════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 7        ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ innovation r…       ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 10       ┆ Diego Estrada       ┆ [\"Diego\",           ┆ [true, false, …     ┆ [\"B-NAME_STUDENT\",  │\n",
       "│          ┆                     ┆ \"Estrada\", … \"      ┆ false]              ┆ \"I-NAME_STUDE…      │\n",
       "│          ┆ Design Thinking A…  ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 16       ┆ Reporting process   ┆ [\"Reporting\",       ┆ [true, false, …     ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆                     ┆ \"process\", … \"      ┆ false]              ┆                     │\n",
       "│          ┆ by Gilberto G…      ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "│ 20       ┆ Design Thinking for ┆ [\"Design\",          ┆ [true, true, …      ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ Innovation          ┆ \"Thinking\", … \"     ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆ …                   ┆ \"]                  ┆                     ┆                     │\n",
       "│ 56       ┆ Assignment:  Visual ┆ [\"Assignment\", \":\", ┆ [false, false, …    ┆ [\"O\", \"O\", … \"O\"]   │\n",
       "│          ┆ ization Refle…      ┆ … \"                 ┆ false]              ┆                     │\n",
       "│          ┆                     ┆                     ┆                     ┆                     │\n",
       "│          ┆                     ┆ \"]                  ┆                     ┆                     │\n",
       "└──────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pl.read_json(f\"{DATA_PATH}/train.json\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 外部データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.022756Z",
     "iopub.status.busy": "2024-03-27T02:17:42.022541Z",
     "iopub.status.idle": "2024-03-27T02:17:42.266395Z",
     "shell.execute_reply": "2024-03-27T02:17:42.265825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_string_to_list(example, col):\n",
    "    # 'your_column_name'は変換したい列の名前に置き換えてください\n",
    "    example[col] = eval(example[col])\n",
    "    return example\n",
    "\n",
    "\n",
    "# external_pii_dataset = (\n",
    "#     load_dataset(\n",
    "#         \"csv\",\n",
    "#         data_files={\"train\": f\"{DATA_PATH}/pii_dataset_fixed.csv\"},\n",
    "#         split=\"train\",\n",
    "#     )\n",
    "#     .select_columns([\"document\", \"labels\", \"text\", \"trailing_whitespace\", \"tokens\"])\n",
    "#     .rename_columns({\"labels\": \"provided_labels\", \"text\": \"full_text\"})\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"provided_labels\"}, num_proc=3)\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"trailing_whitespace\"}, num_proc=3)\n",
    "#     .map(convert_string_to_list, fn_kwargs={\"col\": \"tokens\"}, num_proc=3)\n",
    "#     .map(lambda example: {\"flag\": \"external\"}, num_proc=3)\n",
    "# )\n",
    "\n",
    "moredata_pii_dataset = (\n",
    "    load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\"train\": f\"{DATA_PATH}/moredata_dataset_fixed.csv\"},\n",
    "        split=\"train\",\n",
    "    )\n",
    "    .rename_columns({\"labels\": \"provided_labels\", \"text\": \"full_text\"})\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"provided_labels\"}, num_proc=3)\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"trailing_whitespace\"}, num_proc=3)\n",
    "    .map(convert_string_to_list, fn_kwargs={\"col\": \"tokens\"}, num_proc=3)\n",
    "    .map(lambda example: {\"flag\": \"moredata\"}, num_proc=3)\n",
    ")\n",
    "\n",
    "# mixtral\n",
    "mixtral = (\n",
    "    load_dataset(\n",
    "        \"json\", data_files=f\"{DATA_PATH}/mixtral-8x7b-v1.json\", split=\"train\"\n",
    "    ).rename_column(\"labels\", \"provided_labels\")\n",
    "    # 識別のためのflagを追加\n",
    "    .map(lambda example: {\"flag\": \"mixtral\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.269083Z",
     "iopub.status.busy": "2024-03-27T02:17:42.268861Z",
     "iopub.status.idle": "2024-03-27T02:17:42.274589Z",
     "shell.execute_reply": "2024-03-27T02:17:42.273946Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"train_dataset\")\n",
    "print(train_dataset)\n",
    "\n",
    "# print(\"external_pii_dataset\")\n",
    "# print(external_pii_dataset)\n",
    "\n",
    "print(\"moredata_pii_dataset\")\n",
    "print(moredata_pii_dataset)\n",
    "\n",
    "print(\"mixtral\")\n",
    "print(mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.276847Z",
     "iopub.status.busy": "2024-03-27T02:17:42.276633Z",
     "iopub.status.idle": "2024-03-27T02:17:42.283435Z",
     "shell.execute_reply": "2024-03-27T02:17:42.282778Z"
    }
   },
   "outputs": [],
   "source": [
    "# 外部データと結合\n",
    "train_dataset = concatenate_datasets([train_dataset, mixtral, moredata_pii_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.285821Z",
     "iopub.status.busy": "2024-03-27T02:17:42.285611Z",
     "iopub.status.idle": "2024-03-27T02:17:42.506382Z",
     "shell.execute_reply": "2024-03-27T02:17:42.505705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del mixtral, moredata_pii_dataset\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.509129Z",
     "iopub.status.busy": "2024-03-27T02:17:42.508531Z",
     "iopub.status.idle": "2024-03-27T02:17:42.512984Z",
     "shell.execute_reply": "2024-03-27T02:17:42.512386Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"all train\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.514987Z",
     "iopub.status.busy": "2024-03-27T02:17:42.514800Z",
     "iopub.status.idle": "2024-03-27T02:17:42.519677Z",
     "shell.execute_reply": "2024-03-27T02:17:42.519094Z"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.521906Z",
     "iopub.status.busy": "2024-03-27T02:17:42.521467Z",
     "iopub.status.idle": "2024-03-27T02:17:42.524373Z",
     "shell.execute_reply": "2024-03-27T02:17:42.523784Z"
    }
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "if DEBUG:\n",
    "    train_dataset = train_dataset.select(range(300))\n",
    "    EPOCH = 1\n",
    "    print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:42.526481Z",
     "iopub.status.busy": "2024-03-27T02:17:42.526121Z",
     "iopub.status.idle": "2024-03-27T02:17:43.405276Z",
     "shell.execute_reply": "2024-03-27T02:17:43.404438Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))  # \\nを一つのtokenとして扱う\n",
    "# （これをしない場合、\\nは文字の先頭の_として扱われている。\n",
    "# 前: \\nSaito → \"_Saito\"\n",
    "# 後: \\nSaito → \"\\n\", \"Saito\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.408570Z",
     "iopub.status.busy": "2024-03-27T02:17:43.408340Z",
     "iopub.status.idle": "2024-03-27T02:17:43.412530Z",
     "shell.execute_reply": "2024-03-27T02:17:43.411893Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# labelを変換する\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-EMAIL\",\n",
    "    4: \"I-EMAIL\",\n",
    "    5: \"B-USERNAME\",\n",
    "    6: \"I-USERNAME\",\n",
    "    7: \"B-ID_NUM\",\n",
    "    8: \"I-ID_NUM\",\n",
    "    9: \"B-PHONE_NUM\",\n",
    "    10: \"I-PHONE_NUM\",\n",
    "    11: \"B-URL_PERSONAL\",\n",
    "    12: \"I-URL_PERSONAL\",\n",
    "    13: \"B-STREET_ADDRESS\",\n",
    "    14: \"I-STREET_ADDRESS\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.415453Z",
     "iopub.status.busy": "2024-03-27T02:17:43.415241Z",
     "iopub.status.idle": "2024-03-27T02:17:43.446184Z",
     "shell.execute_reply": "2024-03-27T02:17:43.445431Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    与えられたtokenとlabelから、\n",
    "    今回のtokenizerで区切った場合のtokenとlabelを作成する。\n",
    "    \"\"\"\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        # 文字数分だけ、該当のラベルを追加する\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "        # text -> ['Design', ' ']\n",
    "        # labels -> ['O', 'O', 'O', 'O', 'O', 'O', 'O'] (6文字分 + 空白1文字分)\n",
    "\n",
    "    # actual tokenization\n",
    "    # tokenized = tokenizer(\n",
    "    #     \"\".join(text),\n",
    "    #     return_offsets_mapping=True,\n",
    "    #     max_length=max_length,\n",
    "    #     truncation=True,\n",
    "    # )\n",
    "    tokenized = tokenizer(\n",
    "        \"\".join(text),\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=False,\n",
    "    )\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # offset_mappingの各エントリは、トークンが元のテキストのどの範囲（開始位置と終了位置）にマッピングされるかを示す\n",
    "        # タプルまたはリストで構成されます。\n",
    "\n",
    "        # CLS tokenの対応\n",
    "        # CLSやSEPには必ず(start_idx, end_idx) = (0, 0)が割り当てられる\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # 空白が存在する時は、offset_mappingのstart_idxを+1する\n",
    "        # DeBERTaV2Tokenizerは、空白・改行( と\\n)を文字の先頭に▁としてくっつけるため。\n",
    "        # NOTE: もし空白を▁として文字の先頭にくっつけるないtokenizerの場合は、不要\n",
    "        # if text[start_idx].isspace():\n",
    "        #     start_idx += 1\n",
    "        # \\nはisspace()に該当する。\n",
    "        # special tokenとして扱う場合はstart_idx += 1しないようにするため。\n",
    "        # if text[start_idx] == \" \":\n",
    "        #     start_idx += 1\n",
    "\n",
    "        if text[start_idx].isspace() and text[start_idx] != \"\\n\":\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    # Q: token_labelsは何の長さ？\n",
    "    # A: 今回のtokenizerで区切った時の、tokenに該当するlabel\n",
    "    # 例:\n",
    "    # 与えられたtoken example[\"tokens\"][:10]\n",
    "    # -> ['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n",
    "    # 今回のtokenizerで区切ったtoken　tokenizer.convert_ids_to_tokens(tokenized.input_ids[:10])\n",
    "    # -> ['[CLS]', '▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion', '-', 'Av', 'ril']\n",
    "    # 文字数が違う！！！\n",
    "    # 最初に与えられたtokenとそのlabelだと、今回のtokenizerで区切った場合のラベルが分からない。\n",
    "    # そのため、今回のtokenizerで区切った場合のtokenとラベルを作成した。\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
    "    num_proc=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.449103Z",
     "iopub.status.busy": "2024-03-27T02:17:43.448605Z",
     "iopub.status.idle": "2024-03-27T02:17:43.451608Z",
     "shell.execute_reply": "2024-03-27T02:17:43.450955Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Check aligh token label\n",
    "# for input_id, label in zip(train_dataset[\"input_ids\"][0], train_dataset[\"labels\"][0]):\n",
    "#     print(str(tokenizer.convert_ids_to_tokens(input_id)), id2label[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.454052Z",
     "iopub.status.busy": "2024-03-27T02:17:43.453601Z",
     "iopub.status.idle": "2024-03-27T02:17:43.457845Z",
     "shell.execute_reply": "2024-03-27T02:17:43.457209Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-EMAIL',\n",
       " 'I-EMAIL',\n",
       " 'B-USERNAME',\n",
       " 'I-USERNAME',\n",
       " 'B-ID_NUM',\n",
       " 'I-ID_NUM',\n",
       " 'B-PHONE_NUM',\n",
       " 'I-PHONE_NUM',\n",
       " 'B-URL_PERSONAL',\n",
       " 'I-URL_PERSONAL',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'I-STREET_ADDRESS']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id.keys())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.460204Z",
     "iopub.status.busy": "2024-03-27T02:17:43.459760Z",
     "iopub.status.idle": "2024-03-27T02:17:43.463688Z",
     "shell.execute_reply": "2024-03-27T02:17:43.463041Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.466408Z",
     "iopub.status.busy": "2024-03-27T02:17:43.465872Z",
     "iopub.status.idle": "2024-03-27T02:17:43.469061Z",
     "shell.execute_reply": "2024-03-27T02:17:43.468449Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad_to_multiple_of\n",
    "# paddingの際に、指定した数の倍数になるように、各サンプルの長さを揃える\n",
    "# ハードウェアの要件に合致することで、計算効率が良くなる可能性がある\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.471588Z",
     "iopub.status.busy": "2024-03-27T02:17:43.471072Z",
     "iopub.status.idle": "2024-03-27T02:17:43.669879Z",
     "shell.execute_reply": "2024-03-27T02:17:43.669154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.672916Z",
     "iopub.status.busy": "2024-03-27T02:17:43.672414Z",
     "iopub.status.idle": "2024-03-27T02:17:43.678051Z",
     "shell.execute_reply": "2024-03-27T02:17:43.677450Z"
    }
   },
   "outputs": [],
   "source": [
    "def f5_score(precision: float, recall: float, beta: int = 5):\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    DeBERTa Tokenizerでの区切りを用いた評価値\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    # predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # # Unpack nested dictionaries\n",
    "    # final_results = {}\n",
    "    # for key, value in results.items():\n",
    "    #     if isinstance(value, dict):\n",
    "    #         for n, v in value.items():\n",
    "    #             final_results[f\"{key}_{n}\"] = v\n",
    "    #     else:\n",
    "    #         final_results[key] = value\n",
    "\n",
    "    # # f5scoreを追加\n",
    "    # final_results[\"f5score\"] = f5_score(\n",
    "    #     results[\"overall_precision\"], results[\"overall_recall\"]\n",
    "    # )\n",
    "\n",
    "    # return final_results\n",
    "\n",
    "    # seqevalのmetrics関数を使用して、精度、再現率、F1スコア、正解率を計算\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"f5score\": f5_score(precision, recall),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:43.680278Z",
     "iopub.status.busy": "2024-03-27T02:17:43.680073Z",
     "iopub.status.idle": "2024-03-27T02:17:48.149497Z",
     "shell.execute_reply": "2024-03-27T02:17:48.148907Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128016, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:48.152140Z",
     "iopub.status.busy": "2024-03-27T02:17:48.151940Z",
     "iopub.status.idle": "2024-03-27T02:17:48.155871Z",
     "shell.execute_reply": "2024-03-27T02:17:48.155319Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'trailing_whitespace', 'tokens', 'provided_labels', 'flag', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],\n",
       "    num_rows: 11162\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:48.157872Z",
     "iopub.status.busy": "2024-03-27T02:17:48.157695Z",
     "iopub.status.idle": "2024-03-27T02:17:48.267950Z",
     "shell.execute_reply": "2024-03-27T02:17:48.267239Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/document_pattern_dict.json\") as f:\n",
    "    document_pattern_dict = json.load(f)\n",
    "\n",
    "\n",
    "def document_id_to_pattern(example) -> str:\n",
    "    return document_pattern_dict[example[\"document\"]]\n",
    "\n",
    "\n",
    "def add_pattern_column(dataset):\n",
    "    # original_datasetに対し、patternを付与する\n",
    "    dataset = dataset.map(\n",
    "        lambda example: {\"pattern\": document_id_to_pattern(example)}, num_proc=3\n",
    "    )\n",
    "    unique_labels = np.unique(dataset[\"pattern\"])\n",
    "\n",
    "    # train_test_splitのstratifyに使うために、ClassLabelに変換する\n",
    "    class_label_feature = ClassLabel(names=unique_labels.tolist())\n",
    "    return dataset.cast_column(\"pattern\", class_label_feature)\n",
    "\n",
    "\n",
    "def train_valid_split(train_dataset):\n",
    "    # 'flag'列が'original'のもののみをテストデータとする\n",
    "    original_dataset = train_dataset.filter(\n",
    "        lambda x: x[\"flag\"] == \"original\", num_proc=3\n",
    "    )\n",
    "    extrenal_dataset = train_dataset.filter(\n",
    "        lambda x: x[\"flag\"] != \"original\", num_proc=3\n",
    "    )\n",
    "\n",
    "    # pattern列を付与する\n",
    "    original_dataset = add_pattern_column(original_dataset)\n",
    "\n",
    "    # pattern列に対してstratifyになるよう分割する\n",
    "    train_split_dataset = original_dataset.train_test_split(\n",
    "        test_size=VALID_DATA_SIZE, seed=SEED, stratify_by_column=\"pattern\"\n",
    "    )\n",
    "\n",
    "    # trainについて、全てOのデータを除外する\n",
    "    train_split_dataset[\"train\"] = train_split_dataset[\"train\"].filter(\n",
    "        lambda x: x[\"pattern\"] != 0, num_proc=3\n",
    "    )\n",
    "\n",
    "    # validについて、全てOのデータを除外する\n",
    "    train_split_dataset[\"test\"] = train_split_dataset[\"test\"].filter(\n",
    "        lambda x: x[\"pattern\"] != 0, num_proc=3\n",
    "    )\n",
    "\n",
    "    # 'flag'列が'original'でないものを訓練データと検証データに分割する\n",
    "    # train_split_dataset = original_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "\n",
    "    # 再結合する\n",
    "    concat_train_dataset = concatenate_datasets(\n",
    "        [train_split_dataset[\"train\"], extrenal_dataset]\n",
    "    )\n",
    "\n",
    "    train_valid_dataset = DatasetDict(\n",
    "        {\"train\": concat_train_dataset, \"valid\": train_split_dataset[\"test\"]}\n",
    "    )\n",
    "    return train_valid_dataset\n",
    "\n",
    "\n",
    "if not DEBUG:\n",
    "    train_valid_dataset = train_valid_split(train_dataset)\n",
    "else:\n",
    "    original_dataset = train_dataset.filter(lambda x: x[\"flag\"] == \"original\")\n",
    "    extrenal_dataset = train_dataset.filter(lambda x: x[\"flag\"] != \"original\")\n",
    "\n",
    "    train_split_dataset = original_dataset.train_test_split(\n",
    "        test_size=VALID_DATA_SIZE, seed=SEED\n",
    "    )\n",
    "\n",
    "    concat_train_dataset = concatenate_datasets(\n",
    "        [train_split_dataset[\"train\"], extrenal_dataset]\n",
    "    )\n",
    "    train_valid_dataset = DatasetDict(\n",
    "        {\"train\": concat_train_dataset, \"valid\": train_split_dataset[\"test\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:48.270786Z",
     "iopub.status.busy": "2024-03-27T02:17:48.270591Z",
     "iopub.status.idle": "2024-03-27T02:17:48.274511Z",
     "shell.execute_reply": "2024-03-27T02:17:48.273960Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:48.276691Z",
     "iopub.status.busy": "2024-03-27T02:17:48.276512Z",
     "iopub.status.idle": "2024-03-27T02:17:48.314534Z",
     "shell.execute_reply": "2024-03-27T02:17:48.313869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LOG_PATH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f5score\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    report_to=REPORT_TO,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    https://discuss.huggingface.co/t/evaluation-became-slower-and-slower-during-trainer-train/8682/7\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,  # add\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:17:48.316898Z",
     "iopub.status.busy": "2024-03-27T02:17:48.316702Z",
     "iopub.status.idle": "2024-03-27T03:26:27.996704Z",
     "shell.execute_reply": "2024-03-27T03:26:27.995840Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7239' max='7239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7239/7239 1:07:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F5score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.613305</td>\n",
       "      <td>0.935589</td>\n",
       "      <td>0.740917</td>\n",
       "      <td>0.996307</td>\n",
       "      <td>0.917054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.867995</td>\n",
       "      <td>0.968953</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>0.964637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='473' max='473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [473/473 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # モデルの学習\n",
    "    trainer.train()\n",
    "    cv_score = trainer.evaluate()[\"eval_f5score\"]\n",
    "    # モデルの保存\n",
    "    trainer.save_model(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:28.001215Z",
     "iopub.status.busy": "2024-03-27T03:26:28.000907Z",
     "iopub.status.idle": "2024-03-27T03:26:29.832798Z",
     "shell.execute_reply": "2024-03-27T03:26:29.832027Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \".\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make CV DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:29.836057Z",
     "iopub.status.busy": "2024-03-27T03:26:29.835570Z",
     "iopub.status.idle": "2024-03-27T03:26:29.840534Z",
     "shell.execute_reply": "2024-03-27T03:26:29.839672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_valid_preds(trainer, valid_dataset):\n",
    "#     \"\"\"\n",
    "#     trainerを用いてvalid_datasetに対する予測を行う\n",
    "#     \"\"\"\n",
    "#     predictions = trainer.predict(valid_dataset).predictions\n",
    "#     preds_final = predictions.argmax(-1)\n",
    "\n",
    "#     return preds_final\n",
    "\n",
    "\n",
    "# def get_valid_preds_with_pp(trainer: Trainer, valid_dataset, threhosld: float):\n",
    "#     \"\"\"\n",
    "#     trainerを用いてvalid_datasetに対する予測を行う\n",
    "#     \"\"\"\n",
    "#     predictions = trainer.predict(valid_dataset).predictions\n",
    "#     pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n",
    "#         predictions.shape[0], predictions.shape[1], 1\n",
    "#     )\n",
    "\n",
    "#     preds = predictions.argmax(-1)\n",
    "#     preds_without_O = pred_softmax[:, :, 1:].argmax(-1) + 1\n",
    "#     O_preds = pred_softmax[:, :, 0]\n",
    "\n",
    "#     preds_final = np.where(O_preds < threhosld, preds_without_O, preds)\n",
    "\n",
    "#     return preds_final\n",
    "\n",
    "\n",
    "def get_valid_preds_with_pp_for_opt(trainer: Trainer, valid_dataset):\n",
    "    \"\"\"\n",
    "    trainerを用いてvalid_datasetに対する予測を行う\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(valid_dataset).predictions\n",
    "    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n",
    "        predictions.shape[0], predictions.shape[1], 1\n",
    "    )\n",
    "\n",
    "    preds = predictions.argmax(-1)\n",
    "    preds_without_O = pred_softmax[:, :, 1:].argmax(-1) + 1\n",
    "    O_preds = pred_softmax[:, :, 0]\n",
    "\n",
    "    return O_preds, preds_without_O, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:29.843269Z",
     "iopub.status.busy": "2024-03-27T03:26:29.842852Z",
     "iopub.status.idle": "2024-03-27T03:26:29.847120Z",
     "shell.execute_reply": "2024-03-27T03:26:29.846407Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx] * len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # tokenized = tokenizer(\n",
    "    #     \"\".join(text),\n",
    "    #     return_offsets_mapping=True,\n",
    "    #     truncation=True,\n",
    "    #     max_length=INFERENCE_MAX_LENGTH,\n",
    "    # )\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)\n",
    "\n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:29.849590Z",
     "iopub.status.busy": "2024-03-27T03:26:29.849151Z",
     "iopub.status.idle": "2024-03-27T03:26:29.857065Z",
     "shell.execute_reply": "2024-03-27T03:26:29.856436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_output_part(preds_final, valid_dataset):\n",
    "    # triplets = []\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    # token_mapはoffsetsの文字列indexが、何番目のtokenの紐付け\n",
    "    # start_idx = 0の時、tokens[token_map[start_idx]] → 'Design'\n",
    "    # start_idx = 1の時、tokens[token_map[start_idx]] → 'Design'\n",
    "    # ・・・\n",
    "    # start_idx = 5の時、tokens[token_map[start_idx]] → 'Design'\n",
    "    # start_idx = 6の時、tokens[token_map[start_idx]] → '\\n\\n'\n",
    "\n",
    "    # 同じtoken_idに、二つの予測結果が入ってしまう問題\n",
    "    # 原因は、「文字列をtokenizerで区切り、別のlabelと予測した場合を、別のtripletとして扱うから」\n",
    "    # 例えば、'kellyharrison@gmail.com'のlabelの予測結果\n",
    "    # '\\nkelly' -> 'B-EMAIL'\n",
    "    # '##harris' -> 'B-EMAIL'\n",
    "    # '##on' -> 'I-NAME_STUDENT'\n",
    "    # '@' -> 'B-EMAIL'\n",
    "    # 'gmail' -> 'B-EMAIL'\n",
    "    # '.' -> 'B-EMAIL'\n",
    "    # 'com' -> 'B-EMAIL'\n",
    "    # 解決策案: 同じtoken_idの場合は、tripletに追加しない\n",
    "\n",
    "    for p, token_map, offsets, tokens, doc in zip(\n",
    "        preds_final,\n",
    "        valid_dataset[\"token_map\"],\n",
    "        valid_dataset[\"offset_mapping\"],\n",
    "        valid_dataset[\"tokens\"],\n",
    "        valid_dataset[\"document\"],\n",
    "    ):\n",
    "        triplets = []\n",
    "        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "            label_pred = id2label[token_pred]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            # TODO: 答えに\\nが入っている場合がありそうだけど、この処理は本当に問題ない？\n",
    "            # while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            #     start_idx += 1\n",
    "\n",
    "            # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "            # while start_idx < len(token_map) and tokens[token_map[start_idx]] == \" \":\n",
    "            #     start_idx += 1\n",
    "\n",
    "            # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "            while (\n",
    "                start_idx < len(token_map)\n",
    "                and tokens[token_map[start_idx]].isspace()\n",
    "                and tokens[token_map[start_idx]] != \"\\n\"\n",
    "            ):\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            # if label_pred != \"O\" and token_id != -1:\n",
    "            if (\n",
    "                label_pred not in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\")\n",
    "                and token_id != -1\n",
    "            ):\n",
    "                # \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\"についてはpostprocessを行う\n",
    "                # if (\n",
    "                #     label_pred\n",
    "                #     not in (\n",
    "                #         \"O\",\n",
    "                #         \"B-EMAIL\",\n",
    "                #         \"B-PHONE_NUM\",\n",
    "                #         \"I-PHONE_NUM\",\n",
    "                #         \"B-URL_PERSONAL\",\n",
    "                #         \"I-URL_PERSONAL\",\n",
    "                #     )\n",
    "                #     and token_id != -1\n",
    "                # ):\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                if triplet not in triplets:\n",
    "                    if (\n",
    "                        len(triplets) >= 1\n",
    "                        and document[-1] == doc\n",
    "                        and token[-1] == token_id\n",
    "                    ):\n",
    "                        continue\n",
    "                    document.append(doc)\n",
    "                    token.append(token_id)\n",
    "                    label.append(label_pred)\n",
    "                    token_str.append(tokens[token_id])\n",
    "                    triplets.append(triplet)\n",
    "\n",
    "    return document, token, label, token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:29.859480Z",
     "iopub.status.busy": "2024-03-27T03:26:29.859054Z",
     "iopub.status.idle": "2024-03-27T03:26:30.407500Z",
     "shell.execute_reply": "2024-03-27T03:26:30.406810Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:30.410909Z",
     "iopub.status.busy": "2024-03-27T03:26:30.410275Z",
     "iopub.status.idle": "2024-03-27T03:26:30.428786Z",
     "shell.execute_reply": "2024-03-27T03:26:30.428173Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset = train_dataset.filter(\n",
    "    lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "    num_proc=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:30.431604Z",
     "iopub.status.busy": "2024-03-27T03:26:30.431040Z",
     "iopub.status.idle": "2024-03-27T03:26:32.633472Z",
     "shell.execute_reply": "2024-03-27T03:26:32.632772Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def get_rulebase(regex: re.Pattern, data, name: str) -> Optional[list[dict]]:\n",
    "    output = []\n",
    "    matches = regex.findall(data[\"full_text\"])\n",
    "\n",
    "    # NOTE: find_spanにおいて、同じ単語を全て見つけてしまうため、重複をなくす\n",
    "    matches = list(dict.fromkeys(matches))\n",
    "\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    matched_spans = []\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans.append(find_span(target, data[\"tokens\"]))\n",
    "\n",
    "    for matched_span in matched_spans:\n",
    "        for one_token_span in matched_span:\n",
    "            for intermediate, token_idx in enumerate(one_token_span):\n",
    "                if intermediate == 0:\n",
    "                    prefix = \"B\"\n",
    "                else:\n",
    "                    prefix = \"I\"\n",
    "\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"document\": data[\"document\"],\n",
    "                        \"token\": token_idx,\n",
    "                        \"label\": f\"{prefix}-{name}\",\n",
    "                        \"token_str\": data[\"tokens\"][token_idx],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "email_regex = re.compile(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\")\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "url_regex = re.compile(\n",
    "    r\"https?://(?:www\\.)?[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+){1,2}/(?:[a-zA-Z0-9-]+/)*(?:[a-zA-Z0-9-]+\\.html|.*\\.php|.*\\.asp|.*\\.jsp|\\?v=[a-zA-Z0-9-_]+|.*\\.htm|user/[a-zA-Z0-9-_]+|watch\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|)?\"\n",
    ")\n",
    "\n",
    "emails_tst = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "\n",
    "for _data in valid_dataset:\n",
    "    # email\n",
    "    if match := get_rulebase(email_regex, _data, \"EMAIL\"):\n",
    "        emails.extend(match)\n",
    "\n",
    "    # phone number\n",
    "    if match := get_rulebase(phone_num_regex, _data, \"PHONE_NUM\"):\n",
    "        phone_nums.extend(match)\n",
    "\n",
    "    # URL\n",
    "    if match := get_rulebase(url_regex, _data, \"URL_PERSONAL\"):\n",
    "        urls.extend(match)\n",
    "\n",
    "# pp_data = [emails, phone_nums, urls]\n",
    "pp_data = [emails, phone_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.636762Z",
     "iopub.status.busy": "2024-03-27T03:26:32.636190Z",
     "iopub.status.idle": "2024-03-27T03:26:32.640458Z",
     "shell.execute_reply": "2024-03-27T03:26:32.639837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_correct_df(train: pl.DataFrame):\n",
    "    # 学習データから、outputと同様のデータフレームを作成する\n",
    "    outputs = []\n",
    "    for document_id, token, label in zip(\n",
    "        train[\"document\"], train[\"tokens\"], train[\"provided_labels\"]\n",
    "    ):\n",
    "        for token, (token_str, label_one) in enumerate(zip(token, label)):\n",
    "            if label_one != \"O\":\n",
    "                outputs.append((document_id, token, label_one, token_str))\n",
    "    return pl.DataFrame(outputs, schema=[\"document\", \"token\", \"label\", \"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.642782Z",
     "iopub.status.busy": "2024-03-27T03:26:32.642586Z",
     "iopub.status.idle": "2024-03-27T03:26:32.646919Z",
     "shell.execute_reply": "2024-03-27T03:26:32.646343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_correct_pred_join_df(\n",
    "    train_correct_df: pl.DataFrame, valid_pred_df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    validで利用したdocumentのみを抽出し、train_correct_dfとvalid_pred_dfを結合して、documentごとに比較できるようにする\n",
    "    \"\"\"\n",
    "    out = train_correct_df.filter(\n",
    "        pl.col(\"document\").is_in(valid_pred_df[\"document\"])\n",
    "    ).join(valid_pred_df, on=[\"document\", \"token\"], how=\"outer\", suffix=\"_pred\")\n",
    "\n",
    "    joined_dfs = []\n",
    "    for document in out[\"document\"].unique().to_list():\n",
    "        if document is None:\n",
    "            continue\n",
    "        joined_df_per_document = out.filter(\n",
    "            (pl.col(\"document\") == document) | (pl.col(\"document_pred\") == document)\n",
    "        )\n",
    "        joined_dfs.append(joined_df_per_document)\n",
    "\n",
    "    return pl.concat(joined_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.649693Z",
     "iopub.status.busy": "2024-03-27T03:26:32.649079Z",
     "iopub.status.idle": "2024-03-27T03:26:32.655868Z",
     "shell.execute_reply": "2024-03-27T03:26:32.655259Z"
    }
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "\n",
    "def calc_f5_score_from_valid_df(\n",
    "    valid_correct_pred_df: pl.DataFrame, train: pl.DataFrame\n",
    ") -> tuple[dict, float]:\n",
    "    # trainのtokenの長さを追加する\n",
    "    pred_df_agg_with_len = (\n",
    "        valid_correct_pred_df.select(\n",
    "            pl.col(\"document_pred\").cast(pl.Int64),\n",
    "            pl.col(\"token_pred\").cast(pl.Int64),\n",
    "            pl.col(\"label_pred\"),\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .sort(\"document_pred\")\n",
    "        .group_by(\"document_pred\")\n",
    "        .agg(\n",
    "            pl.col(\"token_pred\"),\n",
    "            pl.col(\"label_pred\"),\n",
    "        )\n",
    "        .join(\n",
    "            train.with_columns(\n",
    "                pl.col(\"tokens\").map_elements(len).alias(\"tokens_len\"),\n",
    "            ).select([\"document\", \"tokens_len\", \"labels\"]),\n",
    "            left_on=\"document_pred\",\n",
    "            right_on=\"document\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 推論したlabel列をOを含むtoken列へと変換する\n",
    "    label_pred_alls = []\n",
    "    for token_pred, label_pred, tokens_len in zip(\n",
    "        pred_df_agg_with_len[\"token_pred\"],\n",
    "        pred_df_agg_with_len[\"label_pred\"],\n",
    "        pred_df_agg_with_len[\"tokens_len\"],\n",
    "    ):\n",
    "        label_pred_all = [\"O\" for _ in range(tokens_len)]\n",
    "        for token, label in zip(token_pred, label_pred):\n",
    "            label_pred_all[token] = label\n",
    "        label_pred_alls.append(label_pred_all)\n",
    "\n",
    "    actual_pred_df = pred_df_agg_with_len.with_columns(\n",
    "        pl.Series(\"label_pred_all\", label_pred_alls)\n",
    "    ).select([\"labels\", \"label_pred_all\"])\n",
    "\n",
    "    # ログ用\n",
    "    print(\"classification report\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            actual_pred_df[\"labels\"].to_list(),\n",
    "            actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cls_rep_dict = classification_report(\n",
    "        actual_pred_df[\"labels\"].to_list(),\n",
    "        actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "        output_dict=True,\n",
    "    )\n",
    "\n",
    "    f5score_from_valid_df = precision_recall_fscore_support(\n",
    "        actual_pred_df[\"labels\"].to_list(),\n",
    "        actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "        beta=5,\n",
    "        average=\"micro\",\n",
    "    )[2]\n",
    "\n",
    "    # print(\"classification_report\")\n",
    "    # print(cls_rep_dict)\n",
    "    # print(\"f5score_from_valid_df\")\n",
    "    # print(f5score_from_valid_df)\n",
    "\n",
    "    return cls_rep_dict, f5score_from_valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 閾値の最適化をしない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.658140Z",
     "iopub.status.busy": "2024-03-27T03:26:32.657952Z",
     "iopub.status.idle": "2024-03-27T03:26:32.660978Z",
     "shell.execute_reply": "2024-03-27T03:26:32.660425Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # main\n",
    "# valid_dataset = train_dataset.filter(\n",
    "#     lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "#     num_proc=3,\n",
    "# )\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    "# )\n",
    "\n",
    "# # 閾値を緩める後処理を使わない場合\n",
    "# valid_preds = get_valid_preds(trainer, valid_dataset)\n",
    "# # 閾値を緩める後処理を使う場合\n",
    "# # valid_preds = get_valid_preds_with_pp(trainer, valid_dataset, threhosld=0.90)\n",
    "\n",
    "# document, token, label, token_str = get_output_part(valid_preds, valid_dataset)\n",
    "\n",
    "# valid_pred_df = pl.DataFrame(\n",
    "#     [document, token, label, token_str],\n",
    "#     schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    "# )\n",
    "\n",
    "# postprocess_df = pl.concat([pl.DataFrame(val) for val in pp_data if val != []])\n",
    "# valid_pred_df = pl.concat([valid_pred_df, postprocess_df])\n",
    "\n",
    "# train_correct_df = make_correct_df(pl.from_pandas(train_dataset.to_pandas()))\n",
    "# # train_correct_df = make_correct_df(train)\n",
    "\n",
    "# valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.663175Z",
     "iopub.status.busy": "2024-03-27T03:26:32.662994Z",
     "iopub.status.idle": "2024-03-27T03:26:32.665637Z",
     "shell.execute_reply": "2024-03-27T03:26:32.665073Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# postprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.667856Z",
     "iopub.status.busy": "2024-03-27T03:26:32.667685Z",
     "iopub.status.idle": "2024-03-27T03:26:32.670270Z",
     "shell.execute_reply": "2024-03-27T03:26:32.669708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cls_rep_dict, f5score_from_valid_df = calc_f5_score_from_valid_df(\n",
    "#     valid_correct_pred_df, train\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postprocessの閾値の最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:26:32.672418Z",
     "iopub.status.busy": "2024-03-27T03:26:32.672239Z",
     "iopub.status.idle": "2024-03-27T03:27:13.583455Z",
     "shell.execute_reply": "2024-03-27T03:27:13.582694Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d604bc6b284010b56a7b87f7fba70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 先に推論結果を取得する\n",
    "valid_dataset = train_dataset.filter(\n",
    "    lambda example: example[\"document\"] in train_valid_dataset[\"valid\"][\"document\"],\n",
    "    num_proc=3,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=3\n",
    ")\n",
    "\n",
    "train_correct_df = make_correct_df(pl.from_pandas(train_dataset.to_pandas()))\n",
    "\n",
    "O_preds, preds_without_O, preds = get_valid_preds_with_pp_for_opt(\n",
    "    trainer, valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:27:13.586784Z",
     "iopub.status.busy": "2024-03-27T03:27:13.586543Z",
     "iopub.status.idle": "2024-03-27T03:30:16.556993Z",
     "shell.execute_reply": "2024-03-27T03:30:16.556067Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 0.50から0.99まで、0.01刻みで、valid_predsを取得する\n",
    "# TODO: 遅すぎる(10分ぐらいかかりそう)ので、実行を早くする\n",
    "best_f5score_from_valid_df = 0\n",
    "best_thr = None\n",
    "for thr in np.linspace(0.70, 1.00, 20):\n",
    "    # 推論\n",
    "    valid_preds = np.where(O_preds < thr, preds_without_O, preds)\n",
    "\n",
    "    # valid_correct_pred_dfを作成\n",
    "    document, token, label, token_str = get_output_part(\n",
    "        valid_preds, valid_dataset\n",
    "    )  # 遅い10秒くらい\n",
    "\n",
    "    valid_pred_df = pl.DataFrame(\n",
    "        [document, token, label, token_str],\n",
    "        schema=[\"document\", \"token\", \"label\", \"token_str\"],\n",
    "    )\n",
    "\n",
    "    postprocess_df = pl.concat(\n",
    "        [pl.DataFrame(val) for val in [emails, phone_nums] if val != []]\n",
    "    )\n",
    "    valid_pred_df = pl.concat([valid_pred_df, postprocess_df])\n",
    "\n",
    "    valid_correct_pred_df = make_correct_pred_join_df(train_correct_df, valid_pred_df)\n",
    "\n",
    "    cls_rep_dict, f5score_from_valid_df = calc_f5_score_from_valid_df(\n",
    "        valid_correct_pred_df, train\n",
    "    )  # 遅い10秒くらい\n",
    "    print(thr, f5score_from_valid_df)\n",
    "\n",
    "    if best_f5score_from_valid_df < f5score_from_valid_df:\n",
    "        best_thr = thr\n",
    "        best_cls_rep_dict = cls_rep_dict\n",
    "        best_f5score_from_valid_df = f5score_from_valid_df\n",
    "\n",
    "print(f\"最も良い閾値: {best_thr}\")\n",
    "print(f\"最も良い閾値のときのCVスコア: {best_f5score_from_valid_df}\")\n",
    "print(\"最も良い閾値のclf_report\")\n",
    "print(best_cls_rep_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVをwandbにuploadする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:30:16.559934Z",
     "iopub.status.busy": "2024-03-27T03:30:16.559494Z",
     "iopub.status.idle": "2024-03-27T03:30:53.012142Z",
     "shell.execute_reply": "2024-03-27T03:30:53.011391Z"
    }
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    # valid_df\n",
    "    tbl = wandb.Table(data=valid_correct_pred_df.to_pandas())\n",
    "    wandb.log({\"valid_correct_pred_df\": tbl})\n",
    "\n",
    "    # valid score\n",
    "    wandb.run.summary[\"f5score_from_valid_df\"] = f5score_from_valid_df\n",
    "\n",
    "    # classification report\n",
    "    table = wandb.Table(columns=[\"Tag\", \"Precision\", \"Recall\", \"F1Score\", \"Support\"])\n",
    "    for name in cls_rep_dict.keys():\n",
    "        table.add_data(\n",
    "            name,\n",
    "            cls_rep_dict[name][\"precision\"],\n",
    "            cls_rep_dict[name][\"recall\"],\n",
    "            cls_rep_dict[name][\"f1-score\"],\n",
    "            cls_rep_dict[name][\"support\"],\n",
    "        )\n",
    "    wandb.log({\"classification_report\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:30:53.015686Z",
     "iopub.status.busy": "2024-03-27T03:30:53.015145Z",
     "iopub.status.idle": "2024-03-27T03:30:56.019366Z",
     "shell.execute_reply": "2024-03-27T03:30:56.018464Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:30:56.022263Z",
     "iopub.status.busy": "2024-03-27T03:30:56.021493Z",
     "iopub.status.idle": "2024-03-27T03:30:56.038654Z",
     "shell.execute_reply": "2024-03-27T03:30:56.037475Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(\"cp /notebooks/pll_data_detection/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T03:30:56.041799Z",
     "iopub.status.busy": "2024-03-27T03:30:56.040785Z",
     "iopub.status.idle": "2024-03-27T03:31:13.019914Z",
     "shell.execute_reply": "2024-03-27T03:31:13.019291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    if \"_\" in dataset_name:\n",
    "        raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "    dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "    dataset_metadata[\"title\"] = dataset_name\n",
    "    with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "\n",
    "if (not DEBUG) and UPLOAD_DATA:\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:31:13.022586Z",
     "iopub.status.busy": "2024-03-27T03:31:13.022040Z",
     "iopub.status.idle": "2024-03-27T03:31:13.026065Z",
     "shell.execute_reply": "2024-03-27T03:31:13.025430Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T03:31:13.028402Z",
     "iopub.status.busy": "2024-03-27T03:31:13.028017Z",
     "iopub.status.idle": "2024-03-27T03:31:18.788717Z",
     "shell.execute_reply": "2024-03-27T03:31:18.788200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09dab5b9b580412693d4174f45db5cd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "311fc273d5b8435aac3543b82ddac13e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fde77103ca2b472190b47ba2f8c45197",
       "max": 473.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09dab5b9b580412693d4174f45db5cd9",
       "tabbable": null,
       "tooltip": null,
       "value": 473.0
      }
     },
     "7bb0aeca6f7a47fc8dc8786e51d33046": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9babffc9324f452a9a0706fefcd2a124": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9da486904c9d41c5b21cba532783d05c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c13faea6f49d4b56b5f1f0213542c03c",
       "placeholder": "​",
       "style": "IPY_MODEL_7bb0aeca6f7a47fc8dc8786e51d33046",
       "tabbable": null,
       "tooltip": null,
       "value": " 473/473 [00:01&lt;00:00, 572.18 examples/s]"
      }
     },
     "c13faea6f49d4b56b5f1f0213542c03c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eaff18fbbc974d4bb967a1502bd83e89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9babffc9324f452a9a0706fefcd2a124",
       "placeholder": "​",
       "style": "IPY_MODEL_f6732a9cf6b040d299842789817f783b",
       "tabbable": null,
       "tooltip": null,
       "value": "Map (num_proc=3): 100%"
      }
     },
     "f6732a9cf6b040d299842789817f783b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f89a0cd4050d4458b13ae873586672b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9d604bc6b284010b56a7b87f7fba70c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eaff18fbbc974d4bb967a1502bd83e89",
        "IPY_MODEL_311fc273d5b8435aac3543b82ddac13e",
        "IPY_MODEL_9da486904c9d41c5b21cba532783d05c"
       ],
       "layout": "IPY_MODEL_f89a0cd4050d4458b13ae873586672b1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fde77103ca2b472190b47ba2f8c45197": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
