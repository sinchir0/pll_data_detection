{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/pll_data_detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 'Assignment: Visualization Reflection Submitted by: Nadine Born Course: Design Thinking for Innovation Trail Challenge: To Build or[SEP]\n",
      "[CLS] for Innovation Trail Challenge: To Build or Not to Build An environmental charity wanted to conduct a fundraising campaign to raise[SEP]\n",
      "[CLS] wanted to conduct a fundraising campaign to raise $4 million to build a public path in a busy tourist area of[SEP]\n",
      "[CLS] public path in a busy tourist area of a small town in British Columbia, Canada. They had been gifted a[SEP]\n",
      "[CLS], Canada. They had been gifted a large piece of land by a local landowner, which was a substantial gift[SEP]\n",
      "[CLS] local landowner, which was a substantial gift and prevented them f[SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/pll_data_detection/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "test_string = \"\"\"\n",
    "'Assignment:\\xa0 Visualization\\xa0Reflection\\xa0 Submitted\\xa0by:\\xa0Nadine Born\\xa0 Course:\\xa0 Design\\xa0Thinking\\xa0for\\xa0Innovation\\xa0 \\xa0 Trail\\xa0Challenge:\\xa0To\\xa0Build\\xa0or\\xa0Not\\xa0to\\xa0Build\\xa0 \\xa0 An\\xa0environmental\\xa0charity\\xa0wanted\\xa0to\\xa0conduct\\xa0a\\xa0fundraising\\xa0campaign\\xa0to\\xa0raise\\xa0$4\\xa0million\\xa0to\\xa0build\\xa0a\\xa0 public\\xa0path\\xa0in\\xa0a\\xa0busy\\xa0tourist\\xa0area\\xa0of\\xa0a\\xa0small\\xa0town\\xa0in\\xa0British\\xa0Columbia,\\xa0Canada.\\xa0They\\xa0had\\xa0been\\xa0gifted\\xa0a\\xa0 large\\xa0piece\\xa0of\\xa0land\\xa0by\\xa0a\\xa0local\\xa0landowner,\\xa0which\\xa0was\\xa0a\\xa0substantial\\xa0gift\\xa0and\\xa0prevented\\xa0them\\xa0f\n",
    "\"\"\"\n",
    "\n",
    "tk = tokenizer(\n",
    "    test_string,\n",
    "    max_length=24,\n",
    "    stride=8,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for seq in tk[\"input_ids\"]:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = tokenizer.tokenize(\"'Assignment:\\xa0 Visualization\\xa0R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"xa0\" in tst_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# from tokenizers import AddedToken\n",
    "# tokenizer.add_tokens(AddedToken(\"  \", normalized=False))\n",
    "\n",
    "# tk = tokenizer(\n",
    "#     test_string_2,\n",
    "#     truncation=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: 1, Decoded String: [CLS]\n",
      "Input ID: 463, Decoded String: As\n",
      "Input ID: 262, Decoded String: the\n",
      "Input ID: 6582, Decoded String: consultant\n",
      "Input ID: 270, Decoded String: for\n",
      "Input ID: 262, Decoded String: the\n",
      "Input ID: 797, Decoded String: study\n",
      "Input ID: 261, Decoded String: ,\n",
      "Input ID: 273, Decoded String: I\n",
      "Input ID: 3537, Decoded String: chose\n",
      "Input ID: 128001, Decoded String:   \n",
      "Input ID: 15895, Decoded String: visualization\n",
      "Input ID: 401, Decoded String: because\n",
      "Input ID: 262, Decoded String: the\n",
      "Input ID: 4842, Decoded String: charity\n",
      "Input ID: 330, Decoded String: had\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 1695, Decoded String: firm\n",
      "Input ID: 2133, Decoded String: concept\n",
      "Input ID: 265, Decoded String: of\n",
      "Input ID: 579, Decoded String: why\n",
      "Input ID: 306, Decoded String: they\n",
      "Input ID: 858, Decoded String: needed\n",
      "Input ID: 262, Decoded String: the\n",
      "Input ID: 3707, Decoded String: trail\n",
      "Input ID: 261, Decoded String: ,\n",
      "Input ID: 361, Decoded String: how\n",
      "Input ID: 278, Decoded String: it\n",
      "Input ID: 338, Decoded String: would\n",
      "Input ID: 128001, Decoded String:   \n",
      "Input ID: 1591, Decoded String: benefit\n",
      "Input ID: 262, Decoded String: the\n",
      "Input ID: 1240, Decoded String: town\n",
      "Input ID: 261, Decoded String: ,\n",
      "Input ID: 263, Decoded String: and\n",
      "Input ID: 361, Decoded String: how\n",
      "Input ID: 400, Decoded String: much\n",
      "Input ID: 278, Decoded String: it\n",
      "Input ID: 338, Decoded String: would\n",
      "Input ID: 751, Decoded String: cost\n",
      "Input ID: 304, Decoded String: but\n",
      "Input ID: 858, Decoded String: needed\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 19931, Decoded String: persuasive\n",
      "Input ID: 384, Decoded String: way\n",
      "Input ID: 264, Decoded String: to\n",
      "Input ID: 4550, Decoded String: tie\n",
      "Input ID: 278, Decoded String: it\n",
      "Input ID: 305, Decoded String: all\n",
      "Input ID: 603, Decoded String: together\n",
      "Input ID: 260, Decoded String: .\n",
      "Input ID: 2, Decoded String: [SEP]\n"
     ]
    }
   ],
   "source": [
    "test_string_2 = \"As the consultant for the study, I chose  visualization because the charity had a firm concept of why they needed the trail, how it would  benefit the town, and how much it would cost but needed a persuasive way to tie it all together.\"\n",
    "\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "tokenizer.add_tokens(AddedToken(\"  \", normalized=False))\n",
    "\n",
    "# Tokenize the string\n",
    "tk = tokenizer(\n",
    "    test_string_2,\n",
    "    truncation=False,\n",
    ")\n",
    "\n",
    "# Get the input_ids\n",
    "input_ids = tk[\"input_ids\"]\n",
    "\n",
    "# Decode each input_id\n",
    "decoded_strings = [tokenizer.decode([id]) for id in input_ids]\n",
    "\n",
    "# Print input_ids and corresponding decoded strings\n",
    "for id, decoded_string in zip(input_ids, decoded_strings):\n",
    "    print(f\"Input ID: {id}, Decoded String: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: 1, Decoded String: [CLS]\n",
      "Input ID: 6805, Decoded String: Trail\n",
      "Input ID: 6738, Decoded String: Challenge\n",
      "Input ID: 877, Decoded String: :\n",
      "Input ID: 502, Decoded String: To\n",
      "Input ID: 8124, Decoded String: Build\n",
      "Input ID: 289, Decoded String: or\n",
      "Input ID: 951, Decoded String: Not\n",
      "Input ID: 264, Decoded String: to\n",
      "Input ID: 8124, Decoded String: Build\n",
      "Input ID: 816, Decoded String: An\n",
      "Input ID: 2543, Decoded String: environmental\n",
      "Input ID: 4842, Decoded String: charity\n",
      "Input ID: 849, Decoded String: wanted\n",
      "Input ID: 264, Decoded String: to\n",
      "Input ID: 3360, Decoded String: conduct\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 8403, Decoded String: fundraising\n",
      "Input ID: 1528, Decoded String: campaign\n",
      "Input ID: 264, Decoded String: to\n",
      "Input ID: 2512, Decoded String: raise\n",
      "Input ID: 419, Decoded String: $\n",
      "Input ID: 453, Decoded String: 4\n",
      "Input ID: 705, Decoded String: million\n",
      "Input ID: 264, Decoded String: to\n",
      "Input ID: 1044, Decoded String: build\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 613, Decoded String: public\n",
      "Input ID: 1998, Decoded String: path\n",
      "Input ID: 267, Decoded String: in\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 2418, Decoded String: busy\n",
      "Input ID: 6182, Decoded String: tourist\n",
      "Input ID: 537, Decoded String: area\n",
      "Input ID: 265, Decoded String: of\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 536, Decoded String: small\n",
      "Input ID: 1240, Decoded String: town\n",
      "Input ID: 267, Decoded String: in\n",
      "Input ID: 1668, Decoded String: British\n",
      "Input ID: 4877, Decoded String: Columbia\n",
      "Input ID: 2, Decoded String: [SEP]\n"
     ]
    }
   ],
   "source": [
    "test_string_2 = \"Trail   Challenge :   To   Build   or   Not   to   Build     An   environmental   charity   wanted   to   conduct   a   fundraising   campaign   to   raise   $ 4   million   to   build   a   public   path   in   a   busy   tourist   area   of   a   small   town   in   British   Columbia \"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "# tokenizer.add_tokens(AddedToken(\"  \", normalized=False))\n",
    "\n",
    "# Tokenize the string\n",
    "tk = tokenizer(\n",
    "    test_string_2,\n",
    "    truncation=False,\n",
    ")\n",
    "\n",
    "# Get the input_ids\n",
    "input_ids = tk[\"input_ids\"]\n",
    "\n",
    "# Decode each input_id\n",
    "decoded_strings = [tokenizer.decode([id]) for id in input_ids]\n",
    "\n",
    "# Print input_ids and corresponding decoded strings\n",
    "for id, decoded_string in zip(input_ids, decoded_strings):\n",
    "    print(f\"Input ID: {id}, Decoded String: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: 1, Decoded String: [CLS]\n",
      "Input ID: 55440, Decoded String: Silvia\n",
      "Input ID: 9356, Decoded String: Villa\n",
      "Input ID: 107343, Decoded String: lobo\n",
      "Input ID: 268, Decoded String: s\n",
      "Input ID: 128002, Decoded String: \n",
      "\n",
      "\n",
      "Input ID: 6738, Decoded String: Challenge\n",
      "Input ID: 294, Decoded String: :\n",
      "Input ID: 128002, Decoded String: \n",
      "\n",
      "\n",
      "Input ID: 443, Decoded String: There\n",
      "Input ID: 269, Decoded String: is\n",
      "Input ID: 266, Decoded String: a\n",
      "Input ID: 128001, Decoded String: \n",
      "\n",
      "Input ID: 483, Decoded String: company\n",
      "Input ID: 319, Decoded String: which\n",
      "Input ID: 888, Decoded String: provides\n",
      "Input ID: 2, Decoded String: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# test_string_2 = \"\"\" and   potential   donors   to   gauge   their   interest .\"\"\"\n",
    "test_string_2 = (\n",
    "    \"\"\"Silvia Villalobos\\n\\nChallenge:\\n\\nThere is a\\n company which provides\"\"\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "# from tokenizers import AddedToken\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\\n\", normalized=False))\n",
    "\n",
    "# Tokenize the string\n",
    "tk = tokenizer(\n",
    "    test_string_2,\n",
    "    truncation=False,\n",
    ")\n",
    "\n",
    "# Get the input_ids\n",
    "input_ids = tk[\"input_ids\"]\n",
    "\n",
    "# Decode each input_id\n",
    "decoded_strings = [tokenizer.decode([id]) for id in input_ids]\n",
    "\n",
    "# Print input_ids and corresponding decoded strings\n",
    "for id, decoded_string in zip(input_ids, decoded_strings):\n",
    "    print(f\"Input ID: {id}, Decoded String: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁.\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab from the tokenizer\n",
    "vocab = tokenizer.vocab\n",
    "\n",
    "# Reverse the vocab to get a dict with ids as keys and tokens as values\n",
    "id_to_token = {id: token for token, id in vocab.items()}\n",
    "\n",
    "# Get the token for the id 323\n",
    "token = id_to_token.get(323)\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6647"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"▁analyze\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
