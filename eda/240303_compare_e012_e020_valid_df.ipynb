{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "- exp012とexp020のvalid_dfにおけるclassification_reportを比較する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_json(\"../data/train.json\")\n",
    "data_12 = pl.read_csv(\"../valid_df/exp012.csv\")\n",
    "data_20 = pl.read_csv(\"../valid_df/exp020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全行を表示する\n",
    "pl.Config.set_tbl_rows(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = (\n",
    "    data_12.select(\n",
    "        pl.col(\"document_pred\").replace(\"null\", None).cast(pl.Int64),\n",
    "        pl.col(\"token_pred\").replace(\"null\", None).cast(pl.Int64),\n",
    "        pl.col(\"label_pred\").replace(\"null\", None),\n",
    "    )\n",
    "    .drop_nulls()\n",
    "    .sort(\"document_pred\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_token_len = train.with_columns(\n",
    "    pl.col(\"tokens\").map_elements(len).alias(\"tokens_len\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_agg_with_len = (\n",
    "    pred_df.group_by(\"document_pred\")\n",
    "    .agg(\n",
    "        pl.col(\"token_pred\"),\n",
    "        pl.col(\"label_pred\"),\n",
    "    )\n",
    "    .join(\n",
    "        train_with_token_len.select([\"document\", \"tokens_len\", \"labels\"]),\n",
    "        left_on=\"document_pred\",\n",
    "        right_on=\"document\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論したlabel列を\n",
    "label_pred_alls = []\n",
    "for token_pred, label_pred, tokens_len in zip(\n",
    "    pred_df_agg_with_len[\"token_pred\"],\n",
    "    pred_df_agg_with_len[\"label_pred\"],\n",
    "    pred_df_agg_with_len[\"tokens_len\"],\n",
    "):\n",
    "    label_pred_all = [\"O\" for _ in range(tokens_len)]\n",
    "    for token, label in zip(token_pred, label_pred):\n",
    "        label_pred_all[token] = label\n",
    "    label_pred_alls.append(label_pred_all)\n",
    "\n",
    "actual_pred_df = pred_df_agg_with_len.with_columns(\n",
    "    pl.Series(\"label_pred_all\", label_pred_alls)\n",
    ").select([\"labels\", \"label_pred_all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9074074074074073, 0.9168105929763961)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics.sequence_labeling import precision_recall_fscore_support\n",
    "\n",
    "calculated_f1_score = precision_recall_fscore_support(\n",
    "    actual_pred_df[\"labels\"].to_list(),\n",
    "    actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    beta=1,\n",
    "    average=\"micro\",\n",
    ")[2]\n",
    "\n",
    "calculated_f5_score = precision_recall_fscore_support(\n",
    "    actual_pred_df[\"labels\"].to_list(),\n",
    "    actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    beta=5,\n",
    "    average=\"micro\",\n",
    ")[2]\n",
    "\n",
    "calculated_f1_score, calculated_f5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/pll_data_detection/.venv/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "         EMAIL       0.86      1.00      0.92         6\n",
      "        ID_NUM       0.94      0.89      0.92        19\n",
      "  NAME_STUDENT       0.91      0.92      0.92       222\n",
      "     PHONE_NUM       0.33      1.00      0.50         1\n",
      "STREET_ADDRESS       0.00      0.00      0.00         1\n",
      "  URL_PERSONAL       0.85      1.00      0.92        17\n",
      "      USERNAME       0.00      0.00      0.00         1\n",
      "\n",
      "     micro avg       0.90      0.92      0.91       267\n",
      "     macro avg       0.56      0.69      0.60       267\n",
      "  weighted avg       0.90      0.92      0.91       267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        actual_pred_df[\"labels\"].to_list(),\n",
    "        actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8816568047337279, 0.9407479358912092)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = (\n",
    "    data_20.select(\n",
    "        pl.col(\"document_pred\").replace(\"null\", None).cast(pl.Int64),\n",
    "        pl.col(\"token_pred\").replace(\"null\", None).cast(pl.Int64),\n",
    "        pl.col(\"label_pred\").replace(\"null\", None),\n",
    "    )\n",
    "    .drop_nulls()\n",
    "    .sort(\"document_pred\")\n",
    ")\n",
    "\n",
    "train_with_token_len = train.with_columns(\n",
    "    pl.col(\"tokens\").map_elements(len).alias(\"tokens_len\"),\n",
    ")\n",
    "\n",
    "pred_df_agg_with_len = (\n",
    "    pred_df.group_by(\"document_pred\")\n",
    "    .agg(\n",
    "        pl.col(\"token_pred\"),\n",
    "        pl.col(\"label_pred\"),\n",
    "    )\n",
    "    .join(\n",
    "        train_with_token_len.select([\"document\", \"tokens_len\", \"labels\"]),\n",
    "        left_on=\"document_pred\",\n",
    "        right_on=\"document\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# 推論したlabel列を\n",
    "label_pred_alls = []\n",
    "for token_pred, label_pred, tokens_len in zip(\n",
    "    pred_df_agg_with_len[\"token_pred\"],\n",
    "    pred_df_agg_with_len[\"label_pred\"],\n",
    "    pred_df_agg_with_len[\"tokens_len\"],\n",
    "):\n",
    "    label_pred_all = [\"O\" for _ in range(tokens_len)]\n",
    "    for token, label in zip(token_pred, label_pred):\n",
    "        label_pred_all[token] = label\n",
    "    label_pred_alls.append(label_pred_all)\n",
    "\n",
    "actual_pred_df = pred_df_agg_with_len.with_columns(\n",
    "    pl.Series(\"label_pred_all\", label_pred_alls)\n",
    ").select([\"labels\", \"label_pred_all\"])\n",
    "\n",
    "from seqeval.metrics.sequence_labeling import precision_recall_fscore_support\n",
    "\n",
    "calculated_f1_score = precision_recall_fscore_support(\n",
    "    actual_pred_df[\"labels\"].to_list(),\n",
    "    actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    beta=1,\n",
    "    average=\"micro\",\n",
    ")[2]\n",
    "\n",
    "calculated_f5_score = precision_recall_fscore_support(\n",
    "    actual_pred_df[\"labels\"].to_list(),\n",
    "    actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    beta=5,\n",
    "    average=\"micro\",\n",
    ")[2]\n",
    "\n",
    "calculated_f1_score, calculated_f5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "         EMAIL       0.80      1.00      0.89         4\n",
      "        ID_NUM       0.86      1.00      0.92        18\n",
      "  NAME_STUDENT       0.83      0.97      0.89       268\n",
      "     PHONE_NUM       1.00      1.00      1.00         1\n",
      "STREET_ADDRESS       0.00      0.00      0.00         1\n",
      "  URL_PERSONAL       0.82      0.67      0.74        21\n",
      "      USERNAME       1.00      1.00      1.00         2\n",
      "\n",
      "     micro avg       0.83      0.95      0.88       315\n",
      "     macro avg       0.76      0.80      0.78       315\n",
      "  weighted avg       0.83      0.95      0.88       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        actual_pred_df[\"labels\"].to_list(),\n",
    "        actual_pred_df[\"label_pred_all\"].to_list(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
