{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:20:24.960652Z",
     "iopub.status.busy": "2024-04-13T01:20:24.960361Z",
     "iopub.status.idle": "2024-04-13T01:20:24.972931Z",
     "shell.execute_reply": "2024-04-13T01:20:24.972031Z",
     "shell.execute_reply.started": "2024-04-13T01:20:24.960619Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAINED_MODEL_PATH_1 = \"/kaggle/input/e099-original-fake-data-deberta-v3-large\"\n",
    "TRAINED_MODEL_PATH_1 = \"pll_data_detection/trained_models/e072-fix-tokenize\"\n",
    "\n",
    "TRAINING_MODEL_PATHS = [\n",
    "    TRAINED_MODEL_PATH_1\n",
    "]\n",
    "\n",
    "# weights = [1, 1] # average\n",
    "weights = [1] # average\n",
    "weights_probs = [w / sum(weights) for w in weights]\n",
    "\n",
    "# DATA_PATH = \"/kaggle/input/pii-detection-removal-from-educational-data\"\n",
    "DATA_PATH = \"pll_data_detection/data\"\n",
    "\n",
    "# THRESHOLD = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:20:24.974799Z",
     "iopub.status.busy": "2024-04-13T01:20:24.974508Z",
     "iopub.status.idle": "2024-04-13T01:20:24.986289Z",
     "shell.execute_reply": "2024-04-13T01:20:24.985418Z",
     "shell.execute_reply.started": "2024-04-13T01:20:24.974774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "https://www.kaggle.com/competitions/benetech-making-graphs-accessible/discussion/410130#2258335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:20:25.057259Z",
     "iopub.status.busy": "2024-04-13T01:20:25.056923Z",
     "iopub.status.idle": "2024-04-13T01:20:33.413534Z",
     "shell.execute_reply": "2024-04-13T01:20:33.412399Z",
     "shell.execute_reply.started": "2024-04-13T01:20:25.057230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.37.0\n",
      "Uninstalling transformers-4.37.0:\n",
      "  Successfully uninstalled transformers-4.37.0\n",
      "Found existing installation: datasets 2.1.0\n",
      "Uninstalling datasets-2.1.0:\n",
      "  Successfully uninstalled datasets-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers datasets -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:20:33.416580Z",
     "iopub.status.busy": "2024-04-13T01:20:33.416189Z",
     "iopub.status.idle": "2024-04-13T01:21:20.333556Z",
     "shell.execute_reply": "2024-04-13T01:21:20.332447Z",
     "shell.execute_reply.started": "2024-04-13T01:20:33.416545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Not use pip install for match predict's notebook version\n",
    "!python -m pip install -q --no-index --find-links=/kaggle/input/ner-transformer-library-notebook \\\n",
    "transformers \\\n",
    "seqeval \\\n",
    "datasets \\\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:21:20.336195Z",
     "iopub.status.busy": "2024-04-13T01:21:20.335408Z",
     "iopub.status.idle": "2024-04-13T01:21:50.573531Z",
     "shell.execute_reply": "2024-04-13T01:21:50.572776Z",
     "shell.execute_reply.started": "2024-04-13T01:21:20.336148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 01:21:33.360598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-13 01:21:33.360737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-13 01:21:33.634871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import gc\n",
    "import datasets\n",
    "from scipy.special import softmax\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TokenClassificationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:21:50.575114Z",
     "iopub.status.busy": "2024-04-13T01:21:50.574559Z",
     "iopub.status.idle": "2024-04-13T01:21:51.556403Z",
     "shell.execute_reply": "2024-04-13T01:21:51.555440Z",
     "shell.execute_reply.started": "2024-04-13T01:21:50.575089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:21:51.559611Z",
     "iopub.status.busy": "2024-04-13T01:21:51.559299Z",
     "iopub.status.idle": "2024-04-13T01:21:52.634717Z",
     "shell.execute_reply": "2024-04-13T01:21:52.633564Z",
     "shell.execute_reply.started": "2024-04-13T01:21:51.559583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 13 01:21:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:21:52.636548Z",
     "iopub.status.busy": "2024-04-13T01:21:52.636224Z",
     "iopub.status.idle": "2024-04-13T01:21:52.641494Z",
     "shell.execute_reply": "2024-04-13T01:21:52.640588Z",
     "shell.execute_reply.started": "2024-04-13T01:21:52.636520Z"
    }
   },
   "outputs": [],
   "source": [
    "assert transformers.__version__ == '4.37.2'\n",
    "assert datasets.__version__ == '2.16.1'\n",
    "assert evaluate.__version__ == '0.4.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:21:52.643221Z",
     "iopub.status.busy": "2024-04-13T01:21:52.642839Z",
     "iopub.status.idle": "2024-04-13T01:22:12.878323Z",
     "shell.execute_reply": "2024-04-13T01:22:12.877475Z",
     "shell.execute_reply.started": "2024-04-13T01:21:52.643189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0796ef370b448f8af3c3a0ece830356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files={\"test\": f\"{DATA_PATH}/test.json\"}, split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:12.879821Z",
     "iopub.status.busy": "2024-04-13T01:22:12.879502Z",
     "iopub.status.idle": "2024-04-13T01:22:12.883922Z",
     "shell.execute_reply": "2024-04-13T01:22:12.883024Z",
     "shell.execute_reply.started": "2024-04-13T01:22:12.879794Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(TRAINED_MODEL_PATH)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:12.886088Z",
     "iopub.status.busy": "2024-04-13T01:22:12.885410Z",
     "iopub.status.idle": "2024-04-13T01:22:12.898821Z",
     "shell.execute_reply": "2024-04-13T01:22:12.898008Z",
     "shell.execute_reply.started": "2024-04-13T01:22:12.886051Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)\n",
    "       \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:12.900217Z",
     "iopub.status.busy": "2024-04-13T01:22:12.899943Z",
     "iopub.status.idle": "2024-04-13T01:22:13.531656Z",
     "shell.execute_reply": "2024-04-13T01:22:13.530789Z",
     "shell.execute_reply.started": "2024-04-13T01:22:12.900184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1301c31d0c441cc86690bc4381cf86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_tmp = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATHS[0])\n",
    "test_dataset = test_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer_tmp}, num_proc=1)\n",
    "# 現在は同じtokenizerを使ったモデルしか利用していないためエラーにはならないが、\n",
    "# tokenizerが異なるモデルをensemubleする場合には問題になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:13.533226Z",
     "iopub.status.busy": "2024-04-13T01:22:13.532908Z",
     "iopub.status.idle": "2024-04-13T01:22:13.537402Z",
     "shell.execute_reply": "2024-04-13T01:22:13.536569Z",
     "shell.execute_reply.started": "2024-04-13T01:22:13.533190Z"
    }
   },
   "outputs": [],
   "source": [
    "# preds_final.shape\n",
    "# (10, 2000)\n",
    "# 10はデータ数\n",
    "# Q. この2000という数字はどこから来たのか\n",
    "# A. DataCollatorForTokenClassificationのpad_to_multiple_of=16にて、16の倍数にpaddingされるから。\n",
    "#    len(test_dataset[\"input_ids\"][9]) = 1994であり、これを16の倍数にpaddingすると一番近いのが2000である。\n",
    "\n",
    "# Q. preds_final[0][-10:] -> array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "#    つまり、2000のうち、最後の10個を全て1(=B-NAME_STUDENT)と予測している。問題ないのか？\n",
    "# A. 問題ない。if start_idx >= len(token_map):　breakを行っているため、\n",
    "#    offset_mappingのstart_idxがtoken_mapより長くなる = paddingしているゾーン以降はpredictionに入らないようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:13.539392Z",
     "iopub.status.busy": "2024-04-13T01:22:13.538617Z",
     "iopub.status.idle": "2024-04-13T01:22:13.969041Z",
     "shell.execute_reply": "2024-04-13T01:22:13.968096Z",
     "shell.execute_reply.started": "2024-04-13T01:22:13.539365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenizer_tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:13.970563Z",
     "iopub.status.busy": "2024-04-13T01:22:13.970260Z",
     "iopub.status.idle": "2024-04-13T01:22:13.978013Z",
     "shell.execute_reply": "2024-04-13T01:22:13.977311Z",
     "shell.execute_reply.started": "2024-04-13T01:22:13.970526Z"
    }
   },
   "outputs": [],
   "source": [
    "# collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:13.982123Z",
     "iopub.status.busy": "2024-04-13T01:22:13.981752Z",
     "iopub.status.idle": "2024-04-13T01:22:13.988557Z",
     "shell.execute_reply": "2024-04-13T01:22:13.987775Z",
     "shell.execute_reply.started": "2024-04-13T01:22:13.982087Z"
    }
   },
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     \".\", \n",
    "#     per_device_eval_batch_size=1, \n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "# trainer = Trainer(\n",
    "#     model=model, \n",
    "#     args=args, \n",
    "#     data_collator=collator, \n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:13.989778Z",
     "iopub.status.busy": "2024-04-13T01:22:13.989523Z",
     "iopub.status.idle": "2024-04-13T01:22:13.998761Z",
     "shell.execute_reply": "2024-04-13T01:22:13.997816Z",
     "shell.execute_reply.started": "2024-04-13T01:22:13.989757Z"
    }
   },
   "outputs": [],
   "source": [
    "# # pp なし\n",
    "# # predictions = trainer.predict(test_dataset).predictions\n",
    "# # pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n",
    "# # preds_final = predictions.argmax(-1)\n",
    "\n",
    "# # ppあり\n",
    "\n",
    "# predictions = trainer.predict(test_dataset).predictions\n",
    "# pred_softmax = softmax(predictions, axis=2)\n",
    "\n",
    "# preds = pred_softmax.argmax(-1)\n",
    "# # preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n",
    "# preds_without_O = pred_softmax[:,:,1:].argmax(-1) + 1\n",
    "# # O_preds = pred_softmax[:,:,12]\n",
    "# O_pred_proba = pred_softmax[:,:,0]\n",
    "\n",
    "# preds_final = np.where(O_pred_proba < THRESHOLD, preds_without_O , preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:14.000599Z",
     "iopub.status.busy": "2024-04-13T01:22:14.000034Z",
     "iopub.status.idle": "2024-04-13T01:22:32.992163Z",
     "shell.execute_reply": "2024-04-13T01:22:32.991120Z",
     "shell.execute_reply.started": "2024-04-13T01:22:14.000569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_pred_softmax(model_path: str, test_dataset: datasets.arrow_dataset.Dataset):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        \".\", \n",
    "        per_device_eval_batch_size=1, \n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=args, \n",
    "        data_collator=collator, \n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(test_dataset).predictions\n",
    "    pred_softmax = softmax(predictions, axis=2)\n",
    "    \n",
    "    del model, trainer, collator, tokenizer, args, predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return pred_softmax\n",
    "\n",
    "# pred_softmaxs = [\n",
    "#     make_pred_softmax(model_path, test_dataset)\n",
    "#     for model_path\n",
    "#     in TRAINING_MODEL_PATHS\n",
    "# ]\n",
    "\n",
    "for idx, (weight_prob, model_path) in enumerate(zip(weights_probs, TRAINING_MODEL_PATHS)):\n",
    "    if idx == 0:\n",
    "        pred_softmax = weight_prob * make_pred_softmax(model_path, test_dataset)\n",
    "    else:\n",
    "        pred_softmax += weight_prob * make_pred_softmax(model_path, test_dataset)\n",
    "\n",
    "# assert len(weights) == len(pred_softmaxs)\n",
    "\n",
    "# pred_softmax_old = np.sum(\n",
    "#     [\n",
    "#         weight * one_pred_softmax\n",
    "#         for weight, one_pred_softmax\n",
    "#         in zip(\n",
    "#             [w / sum(weights) for w in weights],\n",
    "#             pred_softmaxs\n",
    "#         )\n",
    "#     ],\n",
    "#     axis=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:32.993763Z",
     "iopub.status.busy": "2024-04-13T01:22:32.993435Z",
     "iopub.status.idle": "2024-04-13T01:22:32.999786Z",
     "shell.execute_reply": "2024-04-13T01:22:32.999029Z",
     "shell.execute_reply.started": "2024-04-13T01:22:32.993715Z"
    }
   },
   "outputs": [],
   "source": [
    "# def postprocess_by_threshold(pred_softmax: np.ndarray, thr: float):\n",
    "#     preds = pred_softmax.argmax(-1)\n",
    "#     preds_without_O = pred_softmax[:,:,1:].argmax(-1) + 1\n",
    "#     O_pred_proba = pred_softmax[:,:,0]\n",
    "\n",
    "#     pred_final = np.where(O_pred_proba < thr, preds_without_O , preds)\n",
    "    \n",
    "#     del preds, preds_without_O, O_pred_proba\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     return pred_final\n",
    "\n",
    "# preds_final = postprocess_by_threshold(pred_softmax, THRESHOLD)\n",
    "preds_final = pred_softmax.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.001195Z",
     "iopub.status.busy": "2024-04-13T01:22:33.000841Z",
     "iopub.status.idle": "2024-04-13T01:22:33.012728Z",
     "shell.execute_reply": "2024-04-13T01:22:33.012013Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.001163Z"
    }
   },
   "outputs": [],
   "source": [
    "# del pred_softmax\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.014231Z",
     "iopub.status.busy": "2024-04-13T01:22:33.013882Z",
     "iopub.status.idle": "2024-04-13T01:22:33.024525Z",
     "shell.execute_reply": "2024-04-13T01:22:33.023619Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.014199Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-EMAIL\",\n",
    "    4: \"I-EMAIL\",\n",
    "    5: \"B-USERNAME\",\n",
    "    6: \"I-USERNAME\",\n",
    "    7: \"B-ID_NUM\",\n",
    "    8: \"I-ID_NUM\",\n",
    "    9: \"B-PHONE_NUM\",\n",
    "    10: \"I-PHONE_NUM\",\n",
    "    11: \"B-URL_PERSONAL\",\n",
    "    12: \"I-URL_PERSONAL\",\n",
    "    13: \"B-STREET_ADDRESS\",\n",
    "    14: \"I-STREET_ADDRESS\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:24:05.163713Z",
     "iopub.status.busy": "2024-04-13T01:24:05.162864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_34/3116561652.py\u001b[0m(13)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mtoken_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  token_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  start_idx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  end_idx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_34/3116561652.py\u001b[0m(15)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m        \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'O'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_34/3116561652.py\u001b[0m(16)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     14 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mtoken_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_34/3116561652.py\u001b[0m(11)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m):\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m    \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mtoken_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "> \u001b[0;32m/tmp/ipykernel_34/3116561652.py\u001b[0m(13)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mtoken_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# triplets = []\n",
    "document, token, label, token_str = [], [], [], []\n",
    "for p, token_map, offsets, tokens, doc in zip(\n",
    "    preds_final,\n",
    "    test_dataset[\"token_map\"],\n",
    "    test_dataset[\"offset_mapping\"],\n",
    "    test_dataset[\"tokens\"],\n",
    "    test_dataset[\"document\"]\n",
    "):\n",
    "    triplets = []\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[token_pred]\n",
    "\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "        # ignore \"\\n\\n\"\n",
    "        # while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "        #    start_idx += 1\n",
    "        \n",
    "        # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "#         while start_idx < len(token_map) and tokens[token_map[start_idx]] == \" \":\n",
    "#             start_idx += 1\n",
    "\n",
    "        # special tokenに\\nを追加した場合は、\\nのタイミングでstart_idxを=1したくないため\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace() and tokens[token_map[start_idx]] != \"\\n\":\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if label_pred != \"O\" and token_id != -1:\n",
    "#         if (\n",
    "#             label_pred not in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\")\n",
    "#             # label_pred not in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\", \"B-URL_PERSONAL\", \"I-URL_PERSONAL\")\n",
    "#             and token_id != -1\n",
    "#         ):\n",
    "            triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "            if triplet not in triplets:\n",
    "                # 一つ前の(document_id, token_id)と今の(document_id, token_id)が同じ場合は、tripletに追加しない\n",
    "                if len(triplets) >= 1:\n",
    "                    if document[-1] == doc and token[-1] == token_id:\n",
    "                        continue\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                triplets.append(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.123121Z",
     "iopub.status.busy": "2024-04-13T01:22:33.122858Z",
     "iopub.status.idle": "2024-04-13T01:22:33.166697Z",
     "shell.execute_reply": "2024-04-13T01:22:33.165805Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.123100Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pl.DataFrame(\n",
    "    [document, token, label, token_str], schema=[\"document\", \"token\", \"label\", \"token_str\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.168000Z",
     "iopub.status.busy": "2024-04-13T01:22:33.167714Z",
     "iopub.status.idle": "2024-04-13T01:22:33.172683Z",
     "shell.execute_reply": "2024-04-13T01:22:33.171558Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.167956Z"
    }
   },
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "\n",
    "# nlp = English()\n",
    "\n",
    "# def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "#     idx = 0\n",
    "#     spans = []\n",
    "#     span = []\n",
    "\n",
    "#     for i, token in enumerate(document):\n",
    "#         if token != target[idx]:\n",
    "#             idx = 0\n",
    "#             span = []\n",
    "#             continue\n",
    "#         span.append(i)\n",
    "#         idx += 1\n",
    "#         if idx == len(target):\n",
    "#             spans.append(span)\n",
    "#             span = []\n",
    "#             idx = 0\n",
    "#             continue\n",
    "\n",
    "#     return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.174328Z",
     "iopub.status.busy": "2024-04-13T01:22:33.174035Z",
     "iopub.status.idle": "2024-04-13T01:22:33.183689Z",
     "shell.execute_reply": "2024-04-13T01:22:33.182715Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.174305Z"
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# from typing import Optional\n",
    "\n",
    "\n",
    "# def get_rulebase(regex: re.Pattern, data, name: str) -> Optional[list[dict]]:\n",
    "#     output = []\n",
    "#     matches = regex.findall(data[\"full_text\"])\n",
    "\n",
    "#     # NOTE: find_spanにおいて、同じ単語を全て見つけてしまうため、重複をなくす\n",
    "#     matches = list(dict.fromkeys(matches))\n",
    "\n",
    "#     if not matches:\n",
    "#         return None\n",
    "\n",
    "#     matched_spans = []\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans.append(find_span(target, data[\"tokens\"]))\n",
    "\n",
    "#     for matched_span in matched_spans:\n",
    "#         for one_token_span in matched_span:\n",
    "#             for intermediate, token_idx in enumerate(one_token_span):\n",
    "#                 if intermediate == 0:\n",
    "#                     prefix = \"B\"\n",
    "#                 else:\n",
    "#                     prefix = \"I\"\n",
    "\n",
    "#                 output.append(\n",
    "#                     {\n",
    "#                         \"document\": data[\"document\"],\n",
    "#                         \"token\": token_idx,\n",
    "#                         \"label\": f\"{prefix}-{name}\",\n",
    "#                         \"token_str\": data[\"tokens\"][token_idx],\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# email_regex = re.compile(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\")\n",
    "# phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "# # url_regex = re.compile(\n",
    "# #     r\"https?://(?:www\\.)?[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+){1,2}/(?:[a-zA-Z0-9-]+/)*(?:[a-zA-Z0-9-]+\\.html|.*\\.php|.*\\.asp|.*\\.jsp|\\?v=[a-zA-Z0-9-_]+|.*\\.htm|user/[a-zA-Z0-9-_]+|watch\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|\\?v=[a-zA-Z0-9-_]+|[a-zA-Z0-9-_]+|)?\"\n",
    "# # )\n",
    "\n",
    "# emails = []\n",
    "# phone_nums = []\n",
    "# # urls = []\n",
    "\n",
    "# for _data in test_dataset:\n",
    "#     # email\n",
    "#     if match := get_rulebase(email_regex, _data, \"EMAIL\"):\n",
    "#         emails.extend(match)\n",
    "\n",
    "#     # phone number\n",
    "#     if match := get_rulebase(phone_num_regex, _data, \"PHONE_NUM\"):\n",
    "#         phone_nums.extend(match)\n",
    "\n",
    "#     # URL\n",
    "# #     if match := get_rulebase(url_regex, _data, \"URL_PERSONAL\"):\n",
    "# #         urls.extend(match)\n",
    "\n",
    "# # pp_data = [emails, phone_nums, urls]\n",
    "# # pp_data = [emails, phone_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.185229Z",
     "iopub.status.busy": "2024-04-13T01:22:33.184932Z",
     "iopub.status.idle": "2024-04-13T01:22:33.199851Z",
     "shell.execute_reply": "2024-04-13T01:22:33.199176Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.185207Z"
    }
   },
   "outputs": [],
   "source": [
    "# # if emails != [] or phone_nums != [] or urls != []:\n",
    "# if emails != [] or phone_nums != []:\n",
    "#     postprocess_df = pl.concat(\n",
    "#         [pl.DataFrame(val) for val in pp_data if val != []]\n",
    "#     )\n",
    "#     output_df = pl.concat([output_df, postprocess_df])\n",
    "    \n",
    "#     del postprocess_df\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.201083Z",
     "iopub.status.busy": "2024-04-13T01:22:33.200796Z",
     "iopub.status.idle": "2024-04-13T01:22:33.306375Z",
     "shell.execute_reply": "2024-04-13T01:22:33.305496Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.201060Z"
    }
   },
   "outputs": [],
   "source": [
    "# output_df = pl.DataFrame(\n",
    "#     [document, token, label, token_str], schema=[\"document\", \"token\", \"label\", \"token_str\"]\n",
    "# )\n",
    "\n",
    "output_df = (\n",
    "    output_df\n",
    "    .select(\n",
    "        pl.col([\"document\", \"token\", \"label\"])\n",
    "    )\n",
    "    .sort(\n",
    "        [\"document\", \"token\"]\n",
    "    )\n",
    "    .with_row_index(\n",
    "        name=\"row_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.307631Z",
     "iopub.status.busy": "2024-04-13T01:22:33.307379Z",
     "iopub.status.idle": "2024-04-13T01:22:33.326108Z",
     "shell.execute_reply": "2024-04-13T01:22:33.325207Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.307610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (27, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>document</th><th>token</th><th>label</th></tr><tr><td>u32</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>7</td><td>9</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>1</td><td>7</td><td>10</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>2</td><td>7</td><td>482</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>3</td><td>7</td><td>483</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>4</td><td>7</td><td>741</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>5</td><td>7</td><td>742</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>6</td><td>10</td><td>0</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>7</td><td>10</td><td>1</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>8</td><td>10</td><td>464</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>9</td><td>10</td><td>465</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>10</td><td>16</td><td>4</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>11</td><td>16</td><td>5</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>15</td><td>56</td><td>13</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>16</td><td>86</td><td>6</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>17</td><td>86</td><td>7</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>18</td><td>93</td><td>0</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>19</td><td>93</td><td>1</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>20</td><td>104</td><td>7</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>21</td><td>104</td><td>8</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>22</td><td>104</td><td>9</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>23</td><td>112</td><td>5</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>24</td><td>112</td><td>6</td><td>&quot;I-NAME_STUDENT…</td></tr><tr><td>25</td><td>123</td><td>32</td><td>&quot;B-NAME_STUDENT…</td></tr><tr><td>26</td><td>123</td><td>33</td><td>&quot;I-NAME_STUDENT…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (27, 4)\n",
       "┌────────┬──────────┬───────┬────────────────┐\n",
       "│ row_id ┆ document ┆ token ┆ label          │\n",
       "│ ---    ┆ ---      ┆ ---   ┆ ---            │\n",
       "│ u32    ┆ i64      ┆ i64   ┆ str            │\n",
       "╞════════╪══════════╪═══════╪════════════════╡\n",
       "│ 0      ┆ 7        ┆ 9     ┆ B-NAME_STUDENT │\n",
       "│ 1      ┆ 7        ┆ 10    ┆ I-NAME_STUDENT │\n",
       "│ 2      ┆ 7        ┆ 482   ┆ B-NAME_STUDENT │\n",
       "│ 3      ┆ 7        ┆ 483   ┆ I-NAME_STUDENT │\n",
       "│ 4      ┆ 7        ┆ 741   ┆ B-NAME_STUDENT │\n",
       "│ …      ┆ …        ┆ …     ┆ …              │\n",
       "│ 22     ┆ 104      ┆ 9     ┆ I-NAME_STUDENT │\n",
       "│ 23     ┆ 112      ┆ 5     ┆ B-NAME_STUDENT │\n",
       "│ 24     ┆ 112      ┆ 6     ┆ I-NAME_STUDENT │\n",
       "│ 25     ┆ 123      ┆ 32    ┆ B-NAME_STUDENT │\n",
       "│ 26     ┆ 123      ┆ 33    ┆ I-NAME_STUDENT │\n",
       "└────────┴──────────┴───────┴────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if output_df.height < 30:\n",
    "    display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T01:22:33.327904Z",
     "iopub.status.busy": "2024-04-13T01:22:33.327253Z",
     "iopub.status.idle": "2024-04-13T01:22:33.352965Z",
     "shell.execute_reply": "2024-04-13T01:22:33.352290Z",
     "shell.execute_reply.started": "2024-04-13T01:22:33.327873Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df.write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4773320,
     "sourceId": 8086175,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 161710222,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
